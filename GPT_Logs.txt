QUERY_1:

".\data\seg_test\seg_test"

".\data\seg_train\seg_train"

From the root directory, those are the data paths. We must first exit the notebook directory in order to reach these! Inside each of these folders are a few folders named with their classes.

Then, we would like to pass these into cripser:

## set the following appropriately
imgfile = "img.jpg"  ## image file name for a 2D example

# load image data
if os.path.isfile(imgfile):
    img2d = np.array(Image.open(imgfile).convert('L'))
else: # if file is not found, load the demo img from skimage
    img2d = skimage.data.camera()
    io.imsave(imgfile, img2d)

#img2d = skimage.transform.resize(img2d,(300,400))

## apply distance transform
img2d_dt = dt(img2d)

fig,axs = plt.subplots(1,2)
axs[0].imshow(img2d)
axs[1].imshow(img2d_dt)
print(img2d.shape, img2d.dtype, f"{np.min(img2d)}--{np.max(img2d)}")

# 2D: compute PH for the V-construction of the original image (pixel value filtration)

start = time.time()
pd = cr.computePH(img2d)
pds = [pd[pd[:,0] == i] for i in range(3)]
print("Betti numbers: ",[len(pds[i]) for i in range(len(pds))])

# compute PH for the T-construction of the original image (pixel value filtration)
pdt = tcr.computePH(img2d)
pdst = [pdt[pdt[:,0] == i] for i in range(3)]
print("Betti numbers: ",[len(pdst[i]) for i in range(len(pdst))])

Then we would like to vectorize this with GUDHI's persistence landscapes:

LS = gd.representations.Landscape(resolution=1000)
L = LS.fit_transform([acX.persistence_intervals_in_dimension(1)])

plt.plot(L[0][:1000])
plt.plot(L[0][1000:2000])
plt.plot(L[0][2000:3000])
plt.title("Landscape")
plt.show()

OUTPUT_1:

import os
from pathlib import Path

# Adjust path relative to notebook location
root_dir = Path("..")  # Move one level up
train_path = root_dir / "data" / "seg_train" / "seg_train"
test_path = root_dir / "data" / "seg_test" / "seg_test"

# Check existence
print(f"Train Path Exists: {train_path.exists()}")
print(f"Test Path Exists: {test_path.exists()}")


from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import skimage.data
from skimage import io
from scipy.ndimage import distance_transform_edt as dt

# Choose one example image from a class folder
sample_img_path = next(train_path.glob("*/*.jpg"))  # Grabs first .jpg in subfolders
print(f"Using image: {sample_img_path}")

# Load image
img2d = np.array(Image.open(sample_img_path).convert('L'))

# Optional resize:
# from skimage.transform import resize
# img2d = resize(img2d, (300, 400))

# Apply distance transform (optional depending on filtration choice)
img2d_dt = dt(img2d)

# Visualize original and distance transform
fig, axs = plt.subplots(1, 2, figsize=(10, 5))
axs[0].imshow(img2d, cmap='gray')
axs[0].set_title("Original")
axs[1].imshow(img2d_dt, cmap='viridis')
axs[1].set_title("Distance Transform")
plt.show()



import cripser as cr
import tcripser as tcr
import time

start = time.time()
pd = cr.computePH(img2d)
pds = [pd[pd[:, 0] == i] for i in range(3)]  # dimensions 0,1,2
print("V-filtration Betti numbers:", [len(pds[i]) for i in range(len(pds))])
print(f"V-filtration time: {time.time() - start:.2f}s")

start = time.time()
pdt = tcr.computePH(img2d)
pdst = [pdt[pdt[:, 0] == i] for i in range(3)]
print("T-filtration Betti numbers:", [len(pdst[i]) for i in range(len(pdst))])
print(f"T-filtration time: {time.time() - start:.2f}s")


import gudhi as gd
from gudhi.representations import Landscape

# Wrap V-filtration into GUDHI format
acX = gd.representations.PersistenceDiagram(pds[1])  # Dimension 1

# Fit landscape
LS = Landscape(resolution=1000)
L = LS.fit_transform([acX.persistence_intervals_])  # or just [pds[1]]

# Plot first 3 layers of the landscape
plt.plot(L[0][:1000], label="Layer 1")
plt.plot(L[0][1000:2000], label="Layer 2")
plt.plot(L[0][2000:3000], label="Layer 3")
plt.title("Persistence Landscape (dim=1)")
plt.legend()
plt.show()



QUERY_2:
We would like to make a few cells, equally concise:
(1) A 2 x 3 grid of the first image from each class using matplotlib labeled only with the class name and the title (Intel Scene Classification Original Images)
(2) A 2 x 3 grid... only with the class name and the title (Persistence Landscape of Intel Scene Classification Images) [Corresponding to the same images in the same positions].
(3) Let's do persistence diagrams as well

OUTPUT_2:

import matplotlib.pyplot as plt

fig, axes = plt.subplots(2, 3, figsize=(12, 8))
fig.suptitle("Intel Scene Classification Original Images", fontsize=16)

for ax, class_name in sorted(train_df["class"].unique())[:6]:
    img_path = train_df[train_df["class"] == class_name].iloc[0]["image"]
    img = Image.open(img_path)
    ax.imshow(img)
    ax.set_title(class_name)
    ax.axis("off")

plt.tight_layout()
plt.show()



fig, axes = plt.subplots(2, 3, figsize=(12, 8))
fig.suptitle("Persistence Landscape of Intel Scene Classification Images", fontsize=16)

for ax, class_name in sorted(train_df["class"].unique())[:6]:
    img_path = train_df[train_df["class"] == class_name].iloc[0]["image"]
    vec = compute_image_topology_vector(img_path, dims=[1], num_landscapes=3, resolution=100)
    for i in range(3):
        ax.plot(vec[i * 100:(i + 1) * 100])
    ax.set_title(class_name)
    ax.axis("off")

plt.tight_layout()
plt.show()

fig, axes = plt.subplots(2, 3, figsize=(12, 8))
fig.suptitle("Persistence Diagram of Intel Scene Classification Images", fontsize=16)

for ax, class_name in zip(axes.flat, sorted(train_df["class"].unique())[:6]):
    img_path = train_df[train_df["class"] == class_name].iloc[0]["image"]
    img = np.array(Image.open(img_path).convert("L"))
    pdgms = compute_persistence_diagrams(img)
    diag = pdgms[1]
    if diag is not None and len(diag) > 0:
        ax.scatter(diag[:, 0], diag[:, 1], s=10)
        ax.plot([0, diag[:, 1].max()], [0, diag[:, 1].max()], 'r--', linewidth=1)
    ax.set_title(class_name)
    ax.axis("equal")

plt.tight_layout()
plt.show()

QUERY_4:

import matplotlib.pyplot as plt
import gudhi

# rips_on_tore3D_1307.pers obtained from write_persistence_diagram method
persistence_file=gudhi.__root_source_dir__ + \
    '/data/persistence_diagram/rips_on_tore3D_1307.pers'
ax = gudhi.plot_persistence_diagram(persistence_file=persistence_file)
# We can modify the title, aspect, etc.
ax.set_title("Persistence diagram of a torus")
ax.set_aspect("equal")  # forces to be square shaped
plt.show()

Hey remember to do it like this! You can make your code more concise!

OUTPUT_4:

import matplotlib.pyplot as plt
import gudhi

fig, axes = plt.subplots(2, 3, figsize=(12, 8))
fig.suptitle("Persistence Diagrams (dim 0, 1, 2) of Intel Scene Classification Images", fontsize=16)

for ax, class_name in zip(axes.flat, sorted(train_df["class"].unique())[:6]):
    img_path = train_df[train_df["class"] == class_name].iloc[0]["image"]
    img = np.array(Image.open(img_path).convert("L"))
    diagrams = compute_persistence_diagrams(img)
    combined = np.vstack([d for d in diagrams if d is not None and len(d) > 0])
    gudhi.plot_persistence_diagram(diagram=combined, axes=ax)
    ax.set_title(class_name)
    ax.set_aspect("equal")

plt.tight_layout()
plt.show()



QUERY_5:

import torch
import torchvision
from torchvision import models,datasets,transforms
import os
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np
#dir(models)
dir = 'flowers'

params = { 'batch_size':10,
           'shuffle':True,
           'num_workers':4 }


transform = transforms.Compose([transforms.Resize(256),
                                transforms.RandomResizedCrop(256), #Augmented
                                transforms.ToTensor(),
                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])
train_dataset = datasets.ImageFolder(os.path.join(dir, 'train'),transform = transform )

transform = transforms.Compose([transforms.Resize(256),
                                transforms.CenterCrop(256),
                                transforms.ToTensor(),
                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])
val_dataset = datasets.ImageFolder(os.path.join(dir, 'val'),transform = transform )

train_dataloader = torch.utils.data.DataLoader(train_dataset, **params)
val_dataloader = torch.utils.data.DataLoader(val_dataset, **params)

class_names = train_dataset.classes

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print('Train dataset = {}\n'.format(len(train_dataset)),'Val dataset = {}'.format(len(val_dataset)))
print('Classes = {}'.format(class_names))
Train dataset = 747
 Val dataset = 203
Classes = ['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']
def train(model, loss_fn, optimizer, num_epochs=25):

    best_acc = 0

    for epoch in range(num_epochs):
        
        print('Epoch {}'.format(epoch+1))
        
        # Train dataset
        model.train()
        train_loss = 0.0
        train_correct = 0
        size = len(train_dataset)
        for inputs, labels in train_dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            optimizer.zero_grad() # zero the gradients
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            loss = loss_fn(outputs, labels)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * inputs.size(0)
            train_correct += torch.sum(preds == labels.data)
            
        train_loss = train_loss / size
        train_acc = train_correct.double() / size
            
        print('Training Loss: {} Acc: {}'.format(train_loss, train_acc))
        
        # Val dataset
        model.eval()
        val_loss = 0.0
        val_correct = 0
        size = len(val_dataset)
        for inputs, labels in val_dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            optimizer.zero_grad() # zero the gradients
            with torch.no_grad():
                outputs = model(inputs)
                _, preds = torch.max(outputs, 1)
                loss = loss_fn(outputs, labels)
            val_loss += loss.item() * inputs.size(0)
            val_correct += torch.sum(preds == labels.data) 

        val_loss = val_loss / size
        val_acc = val_correct.double() / size    

        print('Validation Loss: {} Acc: {}'.format(val_loss, val_acc))    
            
        if val_acc > best_acc:
            best_acc = val_acc

    print('Best val Acc: {}'.format(best_acc))

    return model
net = torchvision.models.densenet121(pretrained=True)
for param in net.parameters():
    param.requires_grad = False # freeze all the weights

ft = net.classifier.in_features # final layer of the densenet
net.classifier = nn.Linear(ft, 5) # new layer according to our dataset with weights unfrozen

net = net.to(device)

loss = nn.CrossEntropyLoss()

# only final layer optimized
optimizer = optim.SGD(net.classifier.parameters(), lr=0.001, momentum=0.9) 
net = train(net,loss,optimizer,num_epochs=15)
Epoch 1
Training Loss: 1.1538266468558764 Acc: 0.5528781793842035
Validation Loss: 0.8560934140177195 Acc: 0.6896551724137931
Epoch 2
Training Loss: 0.7037181134922916 Acc: 0.7643908969210174
Validation Loss: 0.6782933451565616 Acc: 0.7536945812807881
Epoch 3
Training Loss: 0.5456900371605133 Acc: 0.8326639892904952
Validation Loss: 0.6246734359141054 Acc: 0.7733990147783252
Epoch 4
Training Loss: 0.49786207640426383 Acc: 0.8366800535475234
Validation Loss: 0.6302400983025875 Acc: 0.7536945812807881
Epoch 5
Training Loss: 0.4519619855336556 Acc: 0.8487282463186078
Validation Loss: 0.5164090375888524 Acc: 0.812807881773399
Epoch 6
Training Loss: 0.4344684146693752 Acc: 0.8554216867469879
Validation Loss: 0.506716424461656 Acc: 0.8177339901477833
Epoch 7
Training Loss: 0.3968064436113021 Acc: 0.8728246318607764
Validation Loss: 0.49015344113221604 Acc: 0.8078817733990148
Epoch 8
Training Loss: 0.41830842729553164 Acc: 0.8514056224899598
Validation Loss: 0.4867308308631916 Acc: 0.7980295566502463
Epoch 9
Training Loss: 0.36167163048762396 Acc: 0.8781793842034805
Validation Loss: 0.4818796293782483 Acc: 0.8177339901477833
Epoch 10
Training Loss: 0.43155853151078205 Acc: 0.8701472556894243
Validation Loss: 0.47253260816552956 Acc: 0.8226600985221675
Epoch 11
Training Loss: 0.4147716791255567 Acc: 0.8514056224899598
Validation Loss: 0.4835291514035516 Acc: 0.8078817733990148
Epoch 12
Training Loss: 0.34821611797434576 Acc: 0.892904953145917
Validation Loss: 0.4907854848307342 Acc: 0.8177339901477833
Epoch 13
Training Loss: 0.38205297751080397 Acc: 0.8634538152610441
Validation Loss: 0.463269435126206 Acc: 0.8078817733990148
Epoch 14
Training Loss: 0.3649371044144254 Acc: 0.8661311914323963
Validation Loss: 0.5084417253145443 Acc: 0.8029556650246306
Epoch 15
Training Loss: 0.351822870562833 Acc: 0.8862115127175367
Validation Loss: 0.48289324835016223 Acc: 0.8078817733990148
Best val Acc: 0.8226600985221675
net.eval()

fig = plt.figure()

def imshow(inp, title=None):
    """Imshow for Tensor."""
    inp = inp.numpy().transpose((1, 2, 0))
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    inp = std * inp + mean
    inp = np.clip(inp, 0, 1)
    plt.imshow(inp)
    if title is not None:
        plt.title(title)
    plt.pause(0.001)

with torch.no_grad():
    for i, (inputs, labels) in enumerate(val_dataloader):
        inputs = inputs.to(device)
        labels = labels.to(device)
        
        outputs = net(inputs)
        _, preds = torch.max(outputs, 1)
        for j in range(5):
            ax = plt.subplot(1, 1, 1)
            ax.axis('off')
            ax.set_title('predicted: {}  actual: {}'.format(class_names[preds[j]],class_names[labels[j]]))
            imshow(inputs.cpu().data[j])
        break





 


Okay, now using this as a base, we want to program a densenet-121. Here are our methods so far:

def dataloader(directory):
    records = []
    for class_path in sorted([d for d in directory.iterdir() if d.is_dir()]):
        for image_path in class_path.glob("*.jpg"):
            records.append({"image": str(image_path), "class": class_path.name})
    return pd.DataFrame(records)

fig, axes = plt.subplots(2, 3, figsize = (12, 8))
fig.suptitle("Intel Scene Classification Original Images", fontsize=16)

for ax, class_name in zip(axes.flat, sorted(train_df["class"].unique())[:6]):
    img_path = train_df[train_df["class"] == class_name].iloc[0]["image"]
    img = Image.open(img_path)
    ax.imshow(img)
    ax.set_title(class_name)
    ax.axis("off")

plt.tight_layout()
plt.show()

def compute_persistence_diagrams(img2d):
    pd_v = cr.computePH(img2d)
    pd_v = pd_v[:, :3]
    return [pd_v[pd_v[:, 0] == i][:, 1:] for i in range(3)]

def diagram_to_landscape(diag, num_landscapes = 3, resolution = 100):
    L = Landscape(num_landscapes = num_landscapes, resolution = resolution)
    return L.fit_transform([diag])[0]

def compute_image_topology_vector(img_path, dims = [1], num_landscapes = 3, resolution = 100):
    img2d = np.array(Image.open(img_path).convert("L"))
    pds_v = compute_persistence_diagrams(img2d)
    vecs = [diagram_to_landscape(pds_v[dim], num_landscapes, resolution) for dim in dims]
    return np.concatenate(vecs)

all_vectors = [compute_image_topology_vector(row["image"], dims = [1], num_landscapes = 3, resolution = 100)
               for _, row in tqdm(train_df.iterrows(), total = len(train_df))]
vector_df = pd.DataFrame(all_vectors)
full_topo_df = pd.concat([train_df, vector_df], axis = 1)

fig, axes = plt.subplots(2, 3, figsize = (12, 8))
fig.suptitle("Persistence Diagram of Intel Scene Classification Images", fontsize = 16)

for ax, class_name in zip(axes.flat, sorted(train_df["class"].unique())[:6]):
    img_path = train_df[train_df["class"] == class_name].iloc[0]["image"]
    img = np.array(Image.open(img_path).convert("L"))
    pdgms = compute_persistence_diagrams(img)
    diag = pdgms[1]
    ax.scatter(diag[:, 0], diag[:, 1], s = 10)
    ax.plot([0, diag[:, 1].max()], [0, diag[:, 1].max()], 'r--', linewidth = 1)
    ax.set_title(class_name)
    ax.axis("equal")

plt.tight_layout()
plt.show()

fig, axes = plt.subplots(2, 3, figsize = (12, 8))
fig.suptitle("Persistence Landscapes of Intel Scene Classification Images", fontsize=16)

for ax, class_name in zip(axes.flat, sorted(train_df["class"].unique())[:6]):
    img_path = train_df[train_df["class"] == class_name].iloc[0]["image"]
    vec = compute_image_topology_vector(img_path, dims = [1], num_landscapes = 3, resolution = 100)
    for i in range(3):
        ax.plot(vec[i * 100:(i + 1) * 100])
    ax.set_title(class_name)
    ax.axis("off")

plt.tight_layout()
plt.show()



Oh yeah, we want to take the images as input and output the vector (landscape) since we chose resolution they're going to be consistent size so it's easy! Let's implement that.

OUTPUT_5:

import os
import torch
import numpy as np
import torch.nn as nn
import torch.optim as optim
from PIL import Image
from torch.utils.data import Dataset, DataLoader
from torchvision import models, transforms

# Dataset: image → (transformed RGB, persistence landscape vector)
class ImageToLandscapeDataset(Dataset):
    def __init__(self, dataframe, transform=None):
        self.df = dataframe
        self.transform = transform
        self.vec_cols = [col for col in dataframe.columns if isinstance(col, int)]  # only numeric vector columns

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img = Image.open(row["image"]).convert("RGB")
        if self.transform:
            img = self.transform(img)
        vec = torch.tensor(row[self.vec_cols].values.astype(np.float32))
        return img, vec

# Model: DenseNet-121 → 300-dim landscape
class DenseNetToLandscape(nn.Module):
    def __init__(self, out_dim):
        super().__init__()
        self.base = models.densenet121(pretrained=True)
        for param in self.base.parameters():
            param.requires_grad = False  # freeze pretrained backbone
        in_features = self.base.classifier.in_features
        self.base.classifier = nn.Linear(in_features, out_dim)

    def forward(self, x):
        return self.base(x)

# Training loop
def train_landscape_model(model, dataloader, optimizer, loss_fn, device, epochs=10):
    model.to(device)
    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        for images, vectors in dataloader:
            images, vectors = images.to(device), vectors.to(device)
            optimizer.zero_grad()
            preds = model(images)
            loss = loss_fn(preds, vectors)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * images.size(0)
        avg_loss = running_loss / len(dataloader.dataset)
        print(f"Epoch {epoch + 1}: Loss = {avg_loss:.4f}")
    return model

# Device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Image transform (standard ImageNet pretraining transform)
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(256),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

# Prepare dataset and dataloader
dataset = ImageToLandscapeDataset(full_topo_df, transform=transform)
dataloader = DataLoader(dataset, batch_size=16, shuffle=True)

# Define model, optimizer, loss
model = DenseNetToLandscape(out_dim=300)  # 3 landscapes × 100 resolution
optimizer = optim.Adam(model.parameters(), lr=1e-4)
loss_fn = nn.MSELoss()

# Train
model = train_landscape_model(model, dataloader, optimizer, loss_fn, device, epochs=10)



QUERY_6:

How does on perform these steps on an image dataset:

• Dataset Selection: Choose datasets from sources like UCI Machine Learning
Repository, Kaggle, or government open data portals.
• Data Understanding: Check for missing values, outliers, and inconsistencies.
• Exploratory Data Analysis (EDA): Use visualization tools (Matplotlib, Seaborn,
Plotly) to explore distributions and relationships.

There are techniques for this? We can check for images we should remove or something? Or outlieris etc. etc. ? EDA isn't bad, but I have just printed images and done topological preprocessing, but what should we do here on our imaging dataset:

def dataloader(directory):
    records = []
    for class_path in sorted([d for d in directory.iterdir() if d.is_dir()]):
        for image_path in class_path.glob("*.jpg"):
            records.append({"image": str(image_path), "class": class_path.name})
    return pd.DataFrame(records)

train_dir = Path("../data/seg_train/seg_train")
train_df = dataloader(train_dir)

fig, axes = plt.subplots(2, 3, figsize = (12, 8))
fig.suptitle("Intel Scene Classification Original Images", fontsize=16)

for ax, class_name in zip(axes.flat, sorted(train_df["class"].unique())[:6]):
    img_path = train_df[train_df["class"] == class_name].iloc[0]["image"]
    img = Image.open(img_path)
    ax.imshow(img)
    ax.set_title(class_name)
    ax.axis("off")

plt.tight_layout()
plt.show()

OUTPUT_6:

from PIL import UnidentifiedImageError

def check_corrupt_images(df):
    corrupt_paths = []
    for path in df["image"]:
        try:
            with Image.open(path) as img:
                img.verify()  # PIL's lightweight check
        except (UnidentifiedImageError, IOError, OSError):
            corrupt_paths.append(path)
    return corrupt_paths

corrupt_imgs = check_corrupt_images(train_df)
print("Corrupt images:", corrupt_imgs)


sns.countplot(data=train_df, x="class")
plt.title("Class Distribution")
plt.xticks(rotation=45)
plt.show()


import numpy as np

def is_blank(img):
    arr = np.array(img)
    return np.std(arr) < 1e-3  # All pixels nearly same

blanks = []
for idx, row in train_df.iterrows():
    img = Image.open(row["image"])
    if is_blank(img):
        blanks.append(row["image"])

print(f"Blank or nearly uniform images: {len(blanks)}")


train_df["size"] = train_df["image"].apply(lambda x: Image.open(x).size)
train_df["aspect_ratio"] = train_df["size"].apply(lambda s: s[0] / s[1])

# Aspect ratio distribution
sns.histplot(train_df["aspect_ratio"], bins=30)
plt.title("Aspect Ratio Distribution")
plt.show()


img = Image.open(train_df.iloc[0]["image"])
r, g, b = img.split()

plt.hist(np.array(r).ravel(), bins=256, alpha=0.5, label='R')
plt.hist(np.array(g).ravel(), bins=256, alpha=0.5, label='G')
plt.hist(np.array(b).ravel(), bins=256, alpha=0.5, label='B')
plt.legend()
plt.title("Color Histogram for Sample Image")
plt.show()


fig, axes = plt.subplots(1, 5, figsize=(15, 3))
sample_class = "forest"
sample_paths = train_df[train_df["class"] == sample_class]["image"].sample(5)

for ax, img_path in zip(axes, sample_paths):
    img = Image.open(img_path)
    ax.imshow(img)
    ax.axis("off")
plt.suptitle(f"Samples from class: {sample_class}")
plt.show()


import imagehash

def compute_hash(path):
    img = Image.open(path)
    return str(imagehash.average_hash(img))

train_df["hash"] = train_df["image"].apply(compute_hash)
duplicates = train_df[train_df.duplicated("hash")]
print("Potential duplicate images:", duplicates.shape[0])


QUERY_7:

train_df["size"] = train_df["image"].apply(lambda x: Image.open(x).size)
train_df["aspect_ratio"] = train_df["size"].apply(lambda s: s[0] / s[1])

# Aspect ratio distribution
sns.histplot(train_df["aspect_ratio"], bins=30)
plt.title("Aspect Ratio Distribution")
plt.show()

This but EXACT dimensions [a x b]



OUTPUT_7:

from collections import Counter

# Extract exact dimensions
train_df["dimensions"] = train_df["image"].apply(lambda x: f"{Image.open(x).size[0]}x{Image.open(x).size[1]}")

# Count frequency of each dimension
dimension_counts = train_df["dimensions"].value_counts().reset_index()
dimension_counts.columns = ["dimensions", "count"]

# Plot
plt.figure(figsize=(12, 6))
sns.barplot(data=dimension_counts, x="dimensions", y="count")
plt.title("Image Dimension Distribution (Exact WxH)")
plt.xlabel("Dimensions")
plt.ylabel("Count")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


QUERY_8:

This is showing mostly 150 x 150 and some stragglers. Let's filter the dataset to only include the 150 x 150 images (That way we can use it as input for a CNN without resizing any!)

OUTPUT_8:

# First, parse the image size directly
train_df["size"] = train_df["image"].apply(lambda x: Image.open(x).size)

# Filter only 150x150
filtered_df = train_df[train_df["size"] == (150, 150)].reset_index(drop=True)

print(f"Filtered dataset size: {filtered_df.shape}")



QUERY_9:

Somewhere towards the beginning:

def dataloader(directory):
    records = []
    for class_path in sorted([d for d in directory.iterdir() if d.is_dir()]):
        for image_path in class_path.glob("*.jpg"):
            records.append({"image": str(image_path), "class": class_path.name})
    return pd.DataFrame(records)

train_dir = Path("../data/seg_train/seg_train")
train_df = dataloader(train_dir)

fig, axes = plt.subplots(2, 3, figsize = (12, 8))
fig.suptitle("Intel Scene Classification Original Images", fontsize=16)

for ax, class_name in zip(axes.flat, sorted(train_df["class"].unique())[:6]):
    img_path = train_df[train_df["class"] == class_name].iloc[0]["image"]
    img = Image.open(img_path)
    ax.imshow(img)
    ax.set_title(class_name)
    ax.axis("off")

plt.tight_layout()
plt.show()

for sample_class in ["buildings", "forest", "glacier", "mountain", "sea", "street"]:
    fig, axes = plt.subplots(1, 5, figsize=(15, 3))
    sample_paths = train_df[train_df["class"] == sample_class]["image"].sample(5)

    for ax, img_path in zip(axes, sample_paths):
        img = Image.open(img_path)
        ax.imshow(img)
        ax.axis("off")
    plt.suptitle(f"Samples from class: {sample_class}")
    plt.show()

# First, parse the image size directly
train_df["size"] = train_df["image"].apply(lambda x: Image.open(x).size)

# Filter only 150x150
train_df = train_df[train_df["size"] == (150, 150)].reset_index(drop=True)

print(f"Filtered dataset size: {train_df.shape}")

After all of that runs, I want to partition the training set into a training and validation set with an 80/20 split (Of course, we do that split in each class so it's balanced!). That way, later on, we can compute both the train and validation loss (and plot them against each other, loss per epoch):

class ImageToLandscapeDataset(Dataset):
    def __init__(self, dataframe, transform = None):
        self.df = dataframe
        self.transform = transform
        self.vec_cols = [col for col in dataframe.columns if isinstance(col, int)]

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img = Image.open(row["image"]).convert("RGB")
        if self.transform:
            img = self.transform(img)
        vec = torch.tensor(row[self.vec_cols].values.astype(np.float32))
        return img, vec

class DenseNetToLandscape(nn.Module):
    def __init__(self, out_dim):
        super().__init__()
        self.base = models.densenet121(pretrained = True)
        for param in self.base.parameters():
            param.requires_grad = False
        in_features = self.base.classifier.in_features
        self.base.classifier = nn.Linear(in_features, out_dim)

    def forward(self, x):
        return self.base(x)

def train_landscape_model(model, dataloader, optimizer, loss_fn, device, epochs = 10):
    model.to(device)
    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        for images, vectors in dataloader:
            images, vectors = images.to(device), vectors.to(device)
            optimizer.zero_grad()
            preds = model(images)
            loss = loss_fn(preds, vectors)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * images.size(0)
        avg_loss = running_loss / len(dataloader.dataset)
        print(f"Epoch {epoch + 1}: Loss = {avg_loss:.4f}")
    return model

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(256),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

dataset = ImageToLandscapeDataset(full_topo_df, transform = transform)
dataloader = DataLoader(dataset, batch_size = 16, shuffle = True) 

model = DenseNetToLandscape(out_dim = 300) # 3 landscapes (For each color channel) x 100 resolution = 300
optimizer = optim.Adam(model.parameters(), lr = 1e-4)
loss_fn = nn.MSELoss()

model = train_landscape_model(model, dataloader, optimizer, loss_fn, device, epochs = 10)

Finally, after that first query and before that last one (between the two, we must process train and validation separately!):

all_vectors = [compute_image_topology_vector(row["image"], dims = [1], num_landscapes = 3, resolution = 100)
               for _, row in tqdm(train_df.iterrows(), total = len(train_df))]
vector_df = pd.DataFrame(all_vectors)
full_topo_df = pd.concat([train_df, vector_df], axis = 1)

OUTPUT_10:

from sklearn.model_selection import train_test_split

train_list, val_list = [], []

for class_name, group in train_df.groupby("class"):
    train_split, val_split = train_test_split(
        group, test_size=0.2, random_state=42, shuffle=True, stratify=None
    )
    train_list.append(train_split)
    val_list.append(val_split)

train_split_df = pd.concat(train_list).reset_index(drop=True)
val_split_df = pd.concat(val_list).reset_index(drop=True)

print(f"Train split: {train_split_df.shape}, Val split: {val_split_df.shape}")


from tqdm import tqdm

# Train
train_vectors = [compute_image_topology_vector(row["image"], dims=[1], num_landscapes=3, resolution=100)
                 for _, row in tqdm(train_split_df.iterrows(), total=len(train_split_df))]
train_vector_df = pd.DataFrame(train_vectors)
train_full_df = pd.concat([train_split_df, train_vector_df], axis=1)

# Validation
val_vectors = [compute_image_topology_vector(row["image"], dims=[1], num_landscapes=3, resolution=100)
               for _, row in tqdm(val_split_df.iterrows(), total=len(val_split_df))]
val_vector_df = pd.DataFrame(val_vectors)
val_full_df = pd.concat([val_split_df, val_vector_df], axis=1)


train_dataset = ImageToLandscapeDataset(train_full_df, transform=transform)
val_dataset = ImageToLandscapeDataset(val_full_df, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)


def train_landscape_model(model, train_loader, val_loader, optimizer, loss_fn, device, epochs=10):
    model.to(device)
    train_losses, val_losses = [], []

    for epoch in range(epochs):
        model.train()
        train_loss = 0.0
        for images, vectors in train_loader:
            images, vectors = images.to(device), vectors.to(device)
            optimizer.zero_grad()
            preds = model(images)
            loss = loss_fn(preds, vectors)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * images.size(0)
        avg_train_loss = train_loss / len(train_loader.dataset)
        train_losses.append(avg_train_loss)

        # Validation
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for images, vectors in val_loader:
                images, vectors = images.to(device), vectors.to(device)
                preds = model(images)
                loss = loss_fn(preds, vectors)
                val_loss += loss.item() * images.size(0)
        avg_val_loss = val_loss / len(val_loader.dataset)
        val_losses.append(avg_val_loss)

        print(f"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}")

    return model, train_losses, val_losses


model = DenseNetToLandscape(out_dim=300)
optimizer = optim.Adam(model.parameters(), lr=1e-4)
loss_fn = nn.MSELoss()

model, train_losses, val_losses = train_landscape_model(
    model, train_loader, val_loader, optimizer, loss_fn, device, epochs=10
)

# Plotting loss
plt.figure(figsize=(8, 5))
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel("Epoch")
plt.ylabel("MSE Loss")
plt.title("Training vs Validation Loss")
plt.legend()
plt.grid(True)
plt.show()


QUERY_10:

In this entire section, is there anything else we need to do? I mean like are there any more images we want? We have quite a good number of images:

import os
import pandas as pd
import numpy as np
from pathlib import Path
from PIL import Image
import cripser as cr
from gudhi.representations import Landscape
from tqdm import tqdm
import matplotlib.pyplot as plt
from PIL import UnidentifiedImageError
import seaborn as sns
from collections import Counter
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import models, transforms

def dataloader(directory):
    records = []
    for class_path in sorted([d for d in directory.iterdir() if d.is_dir()]):
        for image_path in class_path.glob("*.jpg"):
            records.append({"image": str(image_path), "class": class_path.name})
    return pd.DataFrame(records)

fig, axes = plt.subplots(2, 3, figsize = (12, 8))
fig.suptitle("Intel Scene Classification Original Images", fontsize=16)

for ax, class_name in zip(axes.flat, sorted(train_df["class"].unique())[:6]):
    img_path = train_df[train_df["class"] == class_name].iloc[0]["image"]
    img = Image.open(img_path)
    ax.imshow(img)
    ax.set_title(class_name)
    ax.axis("off")

plt.tight_layout()
plt.show()

def check_corrupt_images(df):
    corrupt_paths = []
    for path in df["image"]:
        try:
            with Image.open(path) as img:
                img.verify()
        except (UnidentifiedImageError, IOError, OSError):
            corrupt_paths.append(path)
    return corrupt_paths

corrupt_imgs = check_corrupt_images(train_df)
print("Corrupt images:", corrupt_imgs)

sns.countplot(data=train_df, x="class")
plt.title("Class Distribution")
plt.xticks(rotation=45)
plt.show()

def is_blank(img):
    arr = np.array(img)
    return np.std(arr) < 1e-3  # We also want to check for images that are ALMOST uniform

blanks = []
for idx, row in train_df.iterrows():
    img = Image.open(row["image"])
    if is_blank(img):
        blanks.append(row["image"])

print(f"Blank or nearly uniform images: {len(blanks)}")

train_df["dimensions"] = train_df["image"].apply(lambda x: f"{Image.open(x).size[0]}x{Image.open(x).size[1]}")

dimension_counts = train_df["dimensions"].value_counts().reset_index()
dimension_counts.columns = ["dimensions", "count"]

plt.figure(figsize=(12, 6))
sns.barplot(data=dimension_counts, x="dimensions", y="count")
plt.title("Image Dimension Distribution (Exact WxH)")
plt.xlabel("Dimensions")
plt.ylabel("Count")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# First, parse the image size directly
train_df["size"] = train_df["image"].apply(lambda x: Image.open(x).size)

# Filter only 150x150
train_df = train_df[train_df["size"] == (150, 150)].reset_index(drop=True)

print(f"Filtered dataset size: {train_df.shape}")

for sample_class in ["buildings", "forest", "glacier", "mountain", "sea", "street"]:
    fig, axes = plt.subplots(1, 5, figsize=(15, 3))
    sample_paths = train_df[train_df["class"] == sample_class]["image"].sample(5)

    for ax, img_path in zip(axes, sample_paths):
        img = Image.open(img_path)
        ax.imshow(img)
        ax.axis("off")
    plt.suptitle(f"Samples from class: {sample_class}")
    plt.show()

from sklearn.model_selection import train_test_split

train_list, val_list = [], []

for class_name, group in train_df.groupby("class"):
    train_split, val_split = train_test_split(
        group, test_size=0.2, random_state=42, shuffle=True, stratify=None
    )
    train_list.append(train_split)
    val_list.append(val_split)

train_split_df = pd.concat(train_list).reset_index(drop=True)
val_split_df = pd.concat(val_list).reset_index(drop=True)

print(f"Train split: {train_split_df.shape}, Val split: {val_split_df.shape}")

def compute_persistence_diagrams(img2d):
    pd_v = cr.computePH(img2d)
    pd_v = pd_v[:, :3]
    return [pd_v[pd_v[:, 0] == i][:, 1:] for i in range(3)]

def diagram_to_landscape(diag, num_landscapes = 3, resolution = 100):
    L = Landscape(num_landscapes = num_landscapes, resolution = resolution)
    return L.fit_transform([diag])[0]

def compute_image_topology_vector(img_path, dims = [1], num_landscapes = 3, resolution = 100):
    img2d = np.array(Image.open(img_path).convert("L"))
    pds_v = compute_persistence_diagrams(img2d)
    vecs = [diagram_to_landscape(pds_v[dim], num_landscapes, resolution) for dim in dims]
    return np.concatenate(vecs)

from tqdm import tqdm

# Train
train_vectors = [compute_image_topology_vector(row["image"], dims=[1], num_landscapes=3, resolution=100)
                 for _, row in tqdm(train_split_df.iterrows(), total=len(train_split_df))]
train_vector_df = pd.DataFrame(train_vectors)
train_full_df = pd.concat([train_split_df, train_vector_df], axis=1)

# Validation
val_vectors = [compute_image_topology_vector(row["image"], dims=[1], num_landscapes=3, resolution=100)
               for _, row in tqdm(val_split_df.iterrows(), total=len(val_split_df))]
val_vector_df = pd.DataFrame(val_vectors)
val_full_df = pd.concat([val_split_df, val_vector_df], axis=1)

fig, axes = plt.subplots(2, 3, figsize = (12, 8))
fig.suptitle("Persistence Diagram of Intel Scene Classification Images", fontsize = 16)

for ax, class_name in zip(axes.flat, sorted(train_df["class"].unique())[:6]):
    img_path = train_df[train_df["class"] == class_name].iloc[0]["image"]
    img = np.array(Image.open(img_path).convert("L"))
    pdgms = compute_persistence_diagrams(img)
    diag = pdgms[1]
    ax.scatter(diag[:, 0], diag[:, 1], s = 10)
    ax.plot([0, diag[:, 1].max()], [0, diag[:, 1].max()], 'r--', linewidth = 1)
    ax.set_title(class_name)
    ax.axis("equal")

plt.tight_layout()
plt.show()

fig, axes = plt.subplots(2, 3, figsize = (12, 8))
fig.suptitle("Persistence Landscapes of Intel Scene Classification Images", fontsize = 16)

for ax, class_name in zip(axes.flat, sorted(train_df["class"].unique())[:6]):
    img_path = train_df[train_df["class"] == class_name].iloc[0]["image"]
    vec = compute_image_topology_vector(img_path, dims = [1], num_landscapes = 3, resolution = 100)
    for i in range(3):
        ax.plot(vec[i * 100:(i + 1) * 100])
    ax.set_title(class_name)
    ax.axis("off")

plt.tight_layout()
plt.show()

class ImageToLandscapeDataset(Dataset):
    def __init__(self, dataframe, transform = None):
        self.df = dataframe
        self.transform = transform
        self.vec_cols = [col for col in dataframe.columns if isinstance(col, int)]

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img = Image.open(row["image"]).convert("RGB")
        if self.transform:
            img = self.transform(img)
        vec = torch.tensor(row[self.vec_cols].values.astype(np.float32))
        return img, vec

class DenseNetToLandscape(nn.Module):
    def __init__(self, out_dim):
        super().__init__()
        self.base = models.densenet121(pretrained = True)
        for param in self.base.parameters():
            param.requires_grad = False
        in_features = self.base.classifier.in_features
        self.base.classifier = nn.Linear(in_features, out_dim)

    def forward(self, x):
        return self.base(x)

from tqdm import tqdm

def train_landscape_model(model, train_loader, val_loader, optimizer, loss_fn, device, epochs=10):
    model.to(device)
    train_losses, val_losses = [], []

    for epoch in range(epochs):
        model.train()
        train_loss = 0.0

        # Wrap the train_loader in tqdm
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1} [Train]", leave=False)
        for images, vectors in pbar:
            images, vectors = images.to(device), vectors.to(device)
            optimizer.zero_grad()
            preds = model(images)
            loss = loss_fn(preds, vectors)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * images.size(0)
            pbar.set_postfix(loss=loss.item())

        avg_train_loss = train_loss / len(train_loader.dataset)
        train_losses.append(avg_train_loss)

        # Validation loop
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            pbar = tqdm(val_loader, desc=f"Epoch {epoch+1} [Val]", leave=False)
            for images, vectors in pbar:
                images, vectors = images.to(device), vectors.to(device)
                preds = model(images)
                loss = loss_fn(preds, vectors)
                val_loss += loss.item() * images.size(0)
                pbar.set_postfix(loss=loss.item())

        avg_val_loss = val_loss / len(val_loader.dataset)
        val_losses.append(avg_val_loss)

        print(f"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}")

    return model, train_losses, val_losses

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(256),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

train_dataset = ImageToLandscapeDataset(train_full_df, transform=transform)
val_dataset = ImageToLandscapeDataset(val_full_df, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)

model = DenseNetToLandscape(out_dim = 300) # 3 landscapes (For each color channel) x 100 resolution = 300
optimizer = optim.Adam(model.parameters(), lr = 1e-4)
loss_fn = nn.MSELoss()

model = DenseNetToLandscape(out_dim=300)
optimizer = optim.Adam(model.parameters(), lr=1e-4)
loss_fn = nn.MSELoss()

model, train_losses, val_losses = train_landscape_model(
    model, train_loader, val_loader, optimizer, loss_fn, device, epochs=10
)

# Plotting loss
plt.figure(figsize=(8, 5))
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel("Epoch")
plt.ylabel("MSE Loss")
plt.title("Training vs Validation Loss")
plt.legend()
plt.grid(True)
plt.show()

That is, are there any more useful opportunities for visualizations here within the scope of this project?

OUTPUT_10:

for class_name in sorted(train_df["class"].unique()):
    plt.figure(figsize=(6, 3))
    class_imgs = train_df[train_df["class"] == class_name].sample(5)
    for img_path in class_imgs["image"]:
        vec = compute_image_topology_vector(img_path, dims=[1], num_landscapes=3, resolution=100)
        for i in range(3):
            plt.plot(vec[i * 100:(i + 1) * 100], alpha=0.5)
    plt.title(f"Landscapes for class: {class_name}")
    plt.show()


model.eval()
with torch.no_grad():
    for i in range(3):
        img, true_vec = val_dataset[i]
        pred_vec = model(img.unsqueeze(0).to(device)).squeeze().cpu().numpy()

        plt.figure(figsize=(10, 3))
        for j in range(3):
            plt.plot(true_vec[j * 100:(j + 1) * 100], label=f"True L{j+1}", linestyle="--")
            plt.plot(pred_vec[j * 100:(j + 1) * 100], label=f"Pred L{j+1}")
        plt.title("Predicted vs Actual Persistence Landscapes")
        plt.legend()
        plt.show()


from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler

vec_cols = [col for col in train_full_df.columns if isinstance(col, int)]
X = StandardScaler().fit_transform(train_full_df[vec_cols])
y = train_full_df["class"]

X_2d = TSNE(n_components=2, random_state=42).fit_transform(X)

plt.figure(figsize=(10, 6))
sns.scatterplot(x=X_2d[:, 0], y=X_2d[:, 1], hue=y, palette="tab10", s=20)
plt.title("t-SNE of Persistence Landscape Vectors")
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()


QUERY_11:

model.eval()
with torch.no_grad():
    for i in range(3):
        img, true_vec = val_dataset[i]
        pred_vec = model(img.unsqueeze(0).to(device)).squeeze().cpu().numpy()

        plt.figure(figsize=(10, 3))
        for j in range(3):
            plt.plot(true_vec[j * 100:(j + 1) * 100], label=f"True L{j+1}", linestyle="--")
            plt.plot(pred_vec[j * 100:(j + 1) * 100], label=f"Pred L{j+1}")
        plt.title("Predicted vs Actual Persistence Landscapes")
        plt.legend()
        plt.show()

SO FIRST OF ALL, IN THAT PREVIOUS THING, I CHANGED THE COLORS BACK TO RGB. NOW, this one, let's do five again (THIS TIME ONE FOR EACH CLASS), and instead of a generic title, gear each one for each class, also make the colors in the true and predicted match, also only have one legend for all five of them and have it only have two items (The dotted being true and the non dashed being the predicted!)


OUTPUT_11:

colors = ["red", "green", "blue"]
class_names = sorted(val_full_df["class"].unique())

fig, axes = plt.subplots(1, 5, figsize=(20, 4))
plt.suptitle("Predicted vs True Persistence Landscapes per Class", fontsize=16)

model.eval()
with torch.no_grad():
    for ax, class_name in zip(axes, class_names[:5]):
        # Sample one image from this class
        sample_row = val_full_df[val_full_df["class"] == class_name].sample(1).iloc[0]
        img = Image.open(sample_row["image"]).convert("RGB")
        img_tensor = transform(img).unsqueeze(0).to(device)
        
        # True vector from DataFrame, pred from model
        true_vec = torch.tensor(sample_row[[col for col in val_full_df.columns if isinstance(col, int)]].values.astype(np.float32))
        pred_vec = model(img_tensor).squeeze().cpu().numpy()
        
        xs = np.arange(100)
        for i in range(3):
            ax.plot(xs, true_vec[i * 100:(i + 1) * 100], color=colors[i], linestyle="--")
            ax.plot(xs, pred_vec[i * 100:(i + 1) * 100], color=colors[i])
        
        ax.set_title(class_name)
        ax.set_xticks([])
        ax.set_yticks([])
        ax.set_frame_on(False)

# Legend (only once)
from matplotlib.lines import Line2D
custom_lines = [
    Line2D([0], [0], color="black", linestyle="--", label="True"),
    Line2D([0], [0], color="black", linestyle="-", label="Predicted")
]
fig.legend(handles=custom_lines, loc="upper right", fontsize=10)
plt.tight_layout()
plt.subplots_adjust(top=0.85, right=0.95)
plt.show()



QUERY_12:

def is_blank(img):
    arr = np.array(img)
    return np.std(arr) < 20  # We also want to check for images that are ALMOST uniform

blanks = []
for idx, row in train_df.iterrows():
    img = Image.open(row["image"])
    if is_blank(img):
        blanks.append(row["image"])

print(f"Blank or nearly uniform images: {len(blanks)}")

Instead of doing this, why don't we plot a histogram of std for each image and display the ten most uniform images?

OUTPUT_12:



QUERY_13:

5 Model Evaluation and Interpretation
• Metrics: Choose appropriate evaluation metrics:
– Regression: RMSE, MAE, R2

Actually, this section is important to us. Since we're doing regression, we can get tabular output with these, right?

4 Model Selection, Training, and Tuning
• Choosing a Model: Try different models such as GLM, Decision Trees, SVMs,
Random Forests, or Gradient Boosting (XGBoost, LightGBM).
• Train-Test Split: Ensure proper data partitioning (e.g., 70% training, 30% test-
ing) for model assessment. We can also do train-validation-test split (e.g., 70-15-15
or 80-10-10) with large datasets for training, model selection/tuning, and testing.
• Cross-Validation: Use k-fold cross-validation for robust model assessment, hy-
perparameter tuning, and model selection.
• Hyperparameter Tuning: Use Grid Search, Random Search, or Bayesian Opti-
mization.
• Regularization: Apply techniques like L1/L2 regularization to prevent overfitting.

Also, from this section, certanily we would love to use hyperparameter tuning here. Also, would we like to use regularization techniques? I doubt it, I think the architecture is not necessarily the main focus, just breadth and explainability!


So given that this is our model, we will probably get multiple plot and table outputs that we would like:

class ImageToLandscapeDataset(Dataset):
    def __init__(self, dataframe, transform = None):
        self.df = dataframe
        self.transform = transform
        self.vec_cols = [col for col in dataframe.columns if isinstance(col, int)]

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img = Image.open(row["image"]).convert("RGB")
        if self.transform:
            img = self.transform(img)
        vec = torch.tensor(row[self.vec_cols].values.astype(np.float32))
        return img, vec

class DenseNetToLandscape(nn.Module):
    def __init__(self, out_dim):
        super().__init__()
        self.base = models.densenet121(pretrained = True)
        for param in self.base.parameters():
            param.requires_grad = False
        in_features = self.base.classifier.in_features
        self.base.classifier = nn.Linear(in_features, out_dim)

    def forward(self, x):
        return self.base(x)

from tqdm import tqdm

def train_landscape_model(model, train_loader, val_loader, optimizer, loss_fn, device, epochs=10):
    model.to(device)
    train_losses, val_losses = [], []

    for epoch in range(epochs):
        model.train()
        train_loss = 0.0

        # Wrap the train_loader in tqdm
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1} [Train]", leave=False)
        for images, vectors in pbar:
            images, vectors = images.to(device), vectors.to(device)
            optimizer.zero_grad()
            preds = model(images)
            loss = loss_fn(preds, vectors)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * images.size(0)
            pbar.set_postfix(loss=loss.item())

        avg_train_loss = train_loss / len(train_loader.dataset)
        train_losses.append(avg_train_loss)

        # Validation loop
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            pbar = tqdm(val_loader, desc=f"Epoch {epoch+1} [Val]", leave=False)
            for images, vectors in pbar:
                images, vectors = images.to(device), vectors.to(device)
                preds = model(images)
                loss = loss_fn(preds, vectors)
                val_loss += loss.item() * images.size(0)
                pbar.set_postfix(loss=loss.item())

        avg_val_loss = val_loss / len(val_loader.dataset)
        val_losses.append(avg_val_loss)

        print(f"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}")

    return model, train_losses, val_losses

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(256),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

train_dataset = ImageToLandscapeDataset(train_full_df, transform=transform)
val_dataset = ImageToLandscapeDataset(val_full_df, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)

model = DenseNetToLandscape(out_dim = 300) # 3 landscapes (For each color channel) x 100 resolution = 300
optimizer = optim.Adam(model.parameters(), lr = 1e-4)
loss_fn = nn.MSELoss()

model = DenseNetToLandscape(out_dim=300)
optimizer = optim.Adam(model.parameters(), lr=1e-4)
loss_fn = nn.MSELoss()

model, train_losses, val_losses = train_landscape_model(
    model, train_loader, val_loader, optimizer, loss_fn, device, epochs=10
)

# Plotting loss
plt.figure(figsize=(8, 5))
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel("Epoch")
plt.ylabel("MSE Loss")
plt.title("Training vs Validation Loss")
plt.legend()
plt.grid(True)
plt.show()

colors = ["red", "green", "blue"]
class_names = sorted(val_full_df["class"].unique())

fig, axes = plt.subplots(1, 5, figsize=(20, 4))
plt.suptitle("Predicted vs True Persistence Landscapes per Class", fontsize=16)

model.eval()
with torch.no_grad():
    for ax, class_name in zip(axes, class_names[:5]):
        # Sample one image from this class
        sample_row = val_full_df[val_full_df["class"] == class_name].sample(1).iloc[0]
        img = Image.open(sample_row["image"]).convert("RGB")
        img_tensor = transform(img).unsqueeze(0).to(device)
        
        # True vector from DataFrame, pred from model
        true_vec = torch.tensor(sample_row[[col for col in val_full_df.columns if isinstance(col, int)]].values.astype(np.float32))
        pred_vec = model(img_tensor).squeeze().cpu().numpy()
        
        xs = np.arange(100)
        for i in range(3):
            ax.plot(xs, true_vec[i * 100:(i + 1) * 100], color=colors[i], linestyle="--")
            ax.plot(xs, pred_vec[i * 100:(i + 1) * 100], color=colors[i])
        
        ax.set_title(class_name)
        ax.set_xticks([])
        ax.set_yticks([])
        ax.set_frame_on(False)

# Legend (only once)
from matplotlib.lines import Line2D
custom_lines = [
    Line2D([0], [0], color="black", linestyle="--", label="True"),
    Line2D([0], [0], color="black", linestyle="-", label="Predicted")
]
fig.legend(handles=custom_lines, loc="upper right", fontsize=10)
plt.tight_layout()
plt.subplots_adjust(top=0.85, right=0.95)
plt.show()

So let's do hyperparameter tuning (What even are the hyperparameters for this model? Really everything that's interesting and WORTH DOING, don't just include every possible thing). We want all our results in table and plot forms, yep! Using the applicable methods from the list!

OUTPUT_13:

class DenseNetToLandscape(nn.Module):
    def __init__(self, out_dim, unfreeze_top_k=0):
        super().__init__()
        self.base = models.densenet121(pretrained=True)
        
        # Freeze all layers initially
        for param in self.base.parameters():
            param.requires_grad = False

        # Optionally unfreeze top-k layers of features
        if unfreeze_top_k > 0:
            layers = list(self.base.features.children())[::-1]
            count = 0
            for layer in layers:
                for param in layer.parameters():
                    param.requires_grad = True
                count += 1
                if count >= unfreeze_top_k:
                    break

        in_features = self.base.classifier.in_features
        self.base.classifier = nn.Linear(in_features, out_dim)

    def forward(self, x):
        return self.base(x)


from itertools import product

lrs = [1e-4, 3e-4, 1e-3]
decays = [0, 1e-5, 1e-4]
unfreeze_k = [0, 2, 4]  # 0 = frozen, 2 = partial, 4 = more tuning

search_space = list(product(lrs, decays, unfreeze_k))


results = []

for lr, wd, k in search_space:
    print(f"\n--- Training: LR={lr}, WD={wd}, UnfreezeTopK={k} ---")

    model = DenseNetToLandscape(out_dim=300, unfreeze_top_k=k)
    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),
                           lr=lr, weight_decay=wd)

    model, train_losses, val_losses = train_landscape_model(
        model, train_loader, val_loader, optimizer, loss_fn, device, epochs=10
    )

    metrics = evaluate_model(model, val_loader, device)
    metrics.update({
        "LR": lr,
        "WeightDecay": wd,
        "UnfreezeTopK": k,
        "TrainLossFinal": train_losses[-1],
        "ValLossFinal": val_losses[-1],
        "TrainLosses": train_losses,
        "ValLosses": val_losses
    })

    results.append(metrics)


import pandas as pd
metrics_df = pd.DataFrame(results)
summary_df = metrics_df[["LR", "WeightDecay", "UnfreezeTopK", "RMSE", "MAE", "R2", "ValLossFinal"]]
summary_df.sort_values("ValLossFinal")


top_configs = metrics_df.sort_values("ValLossFinal").head(3)

plt.figure(figsize=(10, 6))
for _, row in top_configs.iterrows():
    plt.plot(row["ValLosses"], label=f"LR={row['LR']}, WD={row['WeightDecay']}, k={row['UnfreezeTopK']}")

plt.title("Validation Loss Curves for Top Configs")
plt.xlabel("Epoch")
plt.ylabel("Validation MSE")
plt.legend()
plt.grid(True)
plt.show()


QUERY_13:

import pandas as pd
metrics_df = pd.DataFrame(results)
summary_df = metrics_df[["LR", "WeightDecay", "UnfreezeTopK", "RMSE", "MAE", "R2", "ValLossFinal"]]
summary_df.sort_values("ValLossFinal")

Well this is honestly pretty great. It's giving cool answers! I want to go through with SHAP now, and I want to do SHAP on the best model, then I want to display 5 5 5 5 5 5, that is 5 images each class like this same images, but with the masks on them showing how important each pixel is (Somehow?):

for sample_class in ["buildings", "forest", "glacier", "mountain", "sea", "street"]:
    fig, axes = plt.subplots(1, 5, figsize=(15, 3))
    sample_paths = train_df[train_df["class"] == sample_class]["image"].sample(5)

    for ax, img_path in zip(axes, sample_paths):
        img = Image.open(img_path)
        ax.imshow(img)
        ax.axis("off")
    plt.suptitle(f"Samples from class: {sample_class}")
    plt.show()

OUTPUT_13:

import shap
import torch
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from torchvision import transforms

# SHAP setup for PyTorch
def setup_shap_explainer(model, background_loader, device):
    model.eval()
    background_batch = next(iter(background_loader))[0][:20].to(device)  # A small batch for SHAP background
    explainer = shap.DeepExplainer(model, background_batch)
    return explainer

# Compute SHAP values for one batch of images
def compute_shap_values(explainer, image_batch):
    shap_values = explainer.shap_values(image_batch)
    return shap_values

# Visualize SHAP overlay for a batch of images and their SHAP values
def visualize_shap_images(sample_paths, shap_values, transform, model_input_size=(3, 256, 256)):
    fig, axes = plt.subplots(1, len(sample_paths), figsize=(3 * len(sample_paths), 3))
    for i, (img_path, ax) in enumerate(zip(sample_paths, axes)):
        img = Image.open(img_path).convert("RGB")
        img_tensor = transform(img).unsqueeze(0)
        img_np = np.array(img.resize((model_input_size[2], model_input_size[1]))) / 255.0

        # Combine SHAP across output dimensions (300) using mean absolute value per pixel
        shap_image = np.mean(np.abs(shap_values[0][i]), axis=0).transpose(1, 2, 0)  # CHW to HWC
        shap_image = np.mean(shap_image, axis=2)  # average over channels for heatmap

        ax.imshow(img_np)
        ax.imshow(shap_image, cmap='hot', alpha=0.6)
        ax.axis("off")
    plt.tight_layout()
    plt.show()


for sample_class in ["buildings", "forest", "glacier", "mountain", "sea", "street"]:
    sample_paths = train_df[train_df["class"] == sample_class]["image"].sample(5).tolist()

    # Transform and stack into a batch tensor
    img_batch = torch.stack([
        transform(Image.open(p).convert("RGB")) for p in sample_paths
    ]).to(device)

    # Compute SHAP values
    shap_values = explainer.shap_values(img_batch)

    # Visualize with SHAP masks overlayed
    visualize_shap_images(sample_paths, shap_values, transform)


QUERY_15:

'''
# Fix SHAP incompatibility by disabling in-place ReLUs
def set_relu_to_non_inplace(model):
    for module in model.modules():
        if isinstance(module, torch.nn.ReLU):
            module.inplace = False


model = DenseNetToLandscape(out_dim=300)  # 3 landscapes x 100 resolution
set_relu_to_non_inplace(model)            # 👈 Insert this one line to fix SHAP compatibility
optimizer = optim.Adam(model.parameters(), lr=1e-4)
loss_fn = nn.MSELoss()
'''

But I already have this, what you're telling me to replace it with isn't any better

OUTPUT_15:

import torch.nn.functional as F
from torchvision.models import densenet121

class SHAPSafeDenseNet121(torch.nn.Module):
    def __init__(self, out_dim):
        super().__init__()
        base = densenet121(pretrained=True)

        # Freeze all layers
        for param in base.parameters():
            param.requires_grad = False

        self.features = base.features
        self.pool = torch.nn.AdaptiveAvgPool2d((1, 1))
        self.classifier = torch.nn.Linear(base.classifier.in_features, out_dim)

        # Unfreeze final classifier layer
        for param in self.classifier.parameters():
            param.requires_grad = True

    def forward(self, x):
        features = self.features(x)
        features = F.relu(features.clone(), inplace=False)  # ✅ clone to avoid inplace error on views
        out = self.pool(features)
        out = torch.flatten(out, 1)
        out = self.classifier(out)
        return out


model = SHAPSafeDenseNet121(out_dim=300)
model.to(device)
model.eval()


QUERY_16:

Explaining images:   0%|                                                                     | 0/30 [00:30<?, ?image/s]
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[33], line 18
     15 img_batch = transform(Image.open(img_path).convert("RGB")).unsqueeze(0).to(device)
     17 shap_values = compute_shap_values(explainer, img_batch, top_k=1)  # ✅ use top-3 outputs only
---> 18 visualize_shap_images([img_path], shap_values, transform)

Cell In[32], line 49, in visualize_shap_images(sample_paths, shap_values, transform, model_input_size)
     47 # Aggregate SHAP across top-k output dimensions
     48 per_output_vals = [sv[i] for sv in shap_values]  # [top_k][3, H, W]
---> 49 aggregated = np.mean(np.abs(np.stack(per_output_vals)), axis=0)  # [3, H, W]
     50 shap_heatmap = np.mean(aggregated, axis=0)  # average over channels → [H, W]
     52 ax.imshow(img_np)

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\numpy\core\shape_base.py:443, in stack(arrays, axis, out, dtype, casting)
    372 @array_function_dispatch(_stack_dispatcher)
    373 def stack(arrays, axis=0, out=None, *, dtype=None, casting="same_kind"):
    374     """
    375     Join a sequence of arrays along a new axis.
    376 
   (...)
    441 
    442     """
--> 443     arrays = [asanyarray(arr) for arr in arrays]
    444     if not arrays:
    445         raise ValueError('need at least one array to stack')

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\_tensor.py:1083, in Tensor.__array__(self, dtype)
   1081     return handle_torch_function(Tensor.__array__, (self,), self, dtype=dtype)
   1082 if dtype is None:
-> 1083     return self.numpy()
   1084 else:
   1085     return self.numpy().astype(dtype, copy=False)

TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.

OUTPUT_16:

BOOM 💥 classic PyTorch-on-GPU error — you're trying to pass a CUDA tensor into NumPy without moving it to CPU first. Totally fixable.

🧠 Cause
This line fails:

python
Copy
Edit
np.stack(per_output_vals)
Because per_output_vals are still CUDA tensors, and NumPy can’t deal with that.

✅ Fix
Update this part of your visualize_shap_images function:

python
Copy
Edit
# Aggregate SHAP across top-k output dimensions
per_output_vals = [sv[i].cpu().numpy() for sv in shap_values]  # 👈 move to CPU before stacking
aggregated = np.mean(np.abs(np.stack(per_output_vals)), axis=0)  # [3, H, W]
shap_heatmap = np.mean(aggregated, axis=0)  # average over channels → [H, W]
🧼 Optional Cleanup
To avoid similar issues later, you can also safely detach/clone when you move:

python
Copy
Edit
img_batch = transform(Image.open(img_path).convert("RGB")).unsqueeze(0).to(device).detach()


QUERY_17:

Explaining images:   0%|                                                                     | 0/30 [00:29<?, ?image/s]
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[34], line 80
     77 shap_values = compute_shap_values(explainer, image_tensor, top_k=1)
     79 # Visualize SHAP overlay
---> 80 visualize_shap_images([img_path], shap_values, transform)

Cell In[34], line 48, in visualize_shap_images(sample_paths, shap_values, transform, model_input_size)
     45 img_np = np.array(img_resized) / 255.0
     47 # Convert SHAP tensors to CPU NumPy arrays safely
---> 48 per_output_vals = [sv[i].cpu().numpy() for sv in shap_values]  # [top_k][3, H, W]
     49 aggregated = np.mean(np.abs(np.stack(per_output_vals)), axis=0)  # [3, H, W]
     50 shap_heatmap = np.mean(aggregated, axis=0)  # [H, W]

AttributeError: 'numpy.ndarray' object has no attribute 'cpu'


xplaining images:   0%|                                                                     | 0/30 [00:29<?, ?image/s]
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[37], line 89
     87 shap_values = compute_shap_values(explainer, image_tensor, top_k=1)
     88 # Visualize SHAP overlay for the image
---> 89 visualize_shap_images([img_path], shap_values, transform)

Cell In[37], line 60, in visualize_shap_images(sample_paths, shap_values, transform, model_input_size)
     58     aggregated = np.abs(per_output_vals[0])
     59 else:
---> 60     aggregated = np.mean(np.abs(np.stack(per_output_vals)), axis=0)
     61 shap_heatmap = np.mean(aggregated, axis=0)  # Average over channels → shape [H, W]
     63 ax.imshow(img_np)

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\numpy\core\shape_base.py:449, in stack(arrays, axis, out, dtype, casting)
    447 shapes = {arr.shape for arr in arrays}
    448 if len(shapes) != 1:
--> 449     raise ValueError('all input arrays must have the same shape')
    451 result_ndim = arrays[0].ndim + 1
    452 axis = normalize_axis_index(axis, result_ndim)

ValueError: all input arrays must have the same shape

Yours still gives this error because you didn't actually fix the problem of the improper implementation!

OUTPUT_17:

import shap
import torch
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from torchvision import transforms
from torch.utils.data import DataLoader
from tqdm import tqdm

# ---------- SHAP Setup ----------

def setup_shap_explainer(model, background_loader, device):
    model.eval()
    model.to(device)
    background_batch = next(iter(background_loader))[0][:3].to(device)  # Use 3 background images for speed
    explainer = shap.GradientExplainer(model, background_batch)
    return explainer

# ---------- SHAP Value Computation ----------

def compute_shap_values(explainer, image_batch, top_k=1):
    """
    image_batch: Tensor of shape [B, 3, H, W], already on device.
    Returns SHAP values for top-k output dimensions.
    """
    shap_values = explainer.shap_values(image_batch, ranked_outputs=top_k)
    return shap_values  # List of length = top_k, each element is [B, 3, H, W]

# ---------- SHAP Visualization ----------

def visualize_shap_images(sample_paths, shap_values, transform, model_input_size=(3, 256, 256)):
    """
    Visualizes SHAP overlays.
      - sample_paths: List of image paths (length B)
      - shap_values: List with shape [top_k][B, 3, H, W]. Each element may be a torch.Tensor or a NumPy array.
    """
    fig, axes = plt.subplots(1, len(sample_paths), figsize=(4 * len(sample_paths), 4))
    if len(sample_paths) == 1:
        axes = [axes]  # Ensure iterable

    for i, (img_path, ax) in enumerate(zip(sample_paths, axes)):
        # Load image for display
        img = Image.open(img_path).convert("RGB")
        img_resized = img.resize((model_input_size[2], model_input_size[1]))
        img_np = np.array(img_resized) / 255.0  # Normalize for display

        # Retrieve SHAP values for this image from each output dimension
        per_output_vals = []
        for sv in shap_values:
            val = sv[i]
            if isinstance(val, torch.Tensor):
                val = val.detach().cpu().numpy()
            per_output_vals.append(val)
        
        # Check if all SHAP outputs have the same shape before stacking
        shapes = [np.array(val).shape for val in per_output_vals]
        if len(set(shapes)) == 1:
            aggregated = np.mean(np.abs(np.stack(per_output_vals)), axis=0)
        else:
            # Fallback: use the first output if shapes differ
            aggregated = np.abs(per_output_vals[0])
            
        shap_heatmap = np.mean(aggregated, axis=0)  # Average over channels → [H, W]

        ax.imshow(img_np)
        ax.imshow(shap_heatmap, cmap='hot', alpha=0.6)
        ax.axis("off")

    plt.tight_layout()
    plt.show()

# ---------- Load Background + Set Up Explainer ----------

background_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)
explainer = setup_shap_explainer(model, background_loader, device)

# ---------- Prepare (class, path) Sample List ----------

all_samples = []
for sample_class in ["buildings", "forest", "glacier", "mountain", "sea", "street"]:
    paths = train_df[train_df["class"] == sample_class]["image"].sample(5).tolist()
    all_samples.extend([(sample_class, p) for p in paths])

# ---------- Explain and Visualize (TQDM + Safe) ----------

for class_label, img_path in tqdm(all_samples, desc="Explaining images", unit="image"):
    image_tensor = transform(Image.open(img_path).convert("RGB")).unsqueeze(0).to(device)
    
    # Compute SHAP for top-1 output (faster)
    shap_values = compute_shap_values(explainer, image_tensor, top_k=1)
    
    # Visualize SHAP overlay for the image
    visualize_shap_images([img_path], shap_values, transform)


QUERY_18:

fig, axes = plt.subplots(2, 3, figsize = (12, 8))
fig.suptitle("Persistence Diagram of Intel Scene Classification Images", fontsize = 16)

for ax, class_name in zip(axes.flat, sorted(train_df["class"].unique())[:6]):
    img_path = train_df[train_df["class"] == class_name].iloc[0]["image"]
    img = np.array(Image.open(img_path).convert("L"))
    pdgms = compute_persistence_diagrams(img)
    diag = pdgms[1]
    ax.scatter(diag[:, 0], diag[:, 1], s = 10)
    ax.plot([0, diag[:, 1].max()], [0, diag[:, 1].max()], 'r--', linewidth = 1)
    ax.set_title(class_name)
    ax.axis("equal")

plt.tight_layout()
plt.show()

The same change. Bold, first letter capitalized, Times new roman! Larger font size (These labels shouhld be smaller, ad the size of your previous labels should be the size of the suptitle!)

OUTPUT_18:

fig, axes = plt.subplots(2, 3, figsize=(12, 8))
fig.suptitle("Persistence Diagram of Intel Scene Classification Images", 
             fontsize=24, fontweight='bold', fontname='Times New Roman', color='black')

for ax, class_name in zip(axes.flat, sorted(train_df["class"].unique())[:6]):
    img_path = train_df[train_df["class"] == class_name].iloc[0]["image"]
    img = np.array(Image.open(img_path).convert("L"))
    pdgms = compute_persistence_diagrams(img)
    diag = pdgms[1]
    ax.scatter(diag[:, 0], diag[:, 1], s=10)
    ax.plot([0, diag[:, 1].max()], [0, diag[:, 1].max()], 'r--', linewidth=1)
    # Set subplot title in bold Times New Roman with first letter capitalized
    ax.set_title(class_name.capitalize(), 
                 fontdict={'fontsize':16, 'fontweight':'bold', 'fontname':'Times New Roman', 'color':'black'})
    ax.axis("equal")

plt.tight_layout()
plt.show()


QUERY_19:

fig, axes = plt.subplots(2, 3, figsize = (12, 8))
fig.suptitle("Persistence Landscapes of Intel Scene Classification Images", fontsize = 16)

for ax, class_name in zip(axes.flat, sorted(train_df["class"].unique())[:6]):
    img_path = train_df[train_df["class"] == class_name].iloc[0]["image"]
    vec = compute_image_topology_vector(img_path, dims = [1], num_landscapes = 3, resolution = 100)
    for i in range(3):
        ax.plot(vec[i * 100:(i + 1) * 100])
    ax.set_title(class_name)
    ax.axis("off")

plt.tight_layout()
plt.show()

Same here!

OUTPUT_19:

fig, axes = plt.subplots(2, 3, figsize=(12, 8))
fig.suptitle("Persistence Landscapes of Intel Scene Classification Images", 
             fontsize=24, fontweight='bold', fontname='Times New Roman', color='black')

for ax, class_name in zip(axes.flat, sorted(train_df["class"].unique())[:6]):
    img_path = train_df[train_df["class"] == class_name].iloc[0]["image"]
    vec = compute_image_topology_vector(img_path, dims=[1], num_landscapes=3, resolution=100)
    for i in range(3):
        ax.plot(vec[i * 100:(i + 1) * 100])
    ax.set_title(class_name.capitalize(), 
                 fontdict={'fontsize':16, 'fontweight':'bold', 'fontname':'Times New Roman', 'color':'black'})
    ax.axis("off")

plt.tight_layout()
plt.show()


QUERY_20:

Great, now these don't work anymore since we don't just run a model, we do grid search!

'''
model = DenseNetToLandscape(out_dim=300)
optimizer = optim.Adam(model.parameters(), lr=1e-4)
loss_fn = nn.MSELoss()

model, train_losses, val_losses = train_landscape_model(
    model, train_loader, val_loader, optimizer, loss_fn, device, epochs=10
)

# Plotting loss
plt.figure(figsize=(8, 5))
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel("Epoch")
plt.ylabel("MSE Loss")
plt.title("Training vs Validation Loss")
plt.legend()
plt.grid(True)
plt.show()
'''

AND

'''
LOVE THIS PLOT, DO THIS ON THE BEST PERFORMING MODEL!!!
'''


colors = ["red", "green", "blue"]
class_names = sorted(val_full_df["class"].unique())

fig, axes = plt.subplots(1, 5, figsize=(20, 4))
plt.suptitle("Predicted vs True Persistence Landscapes per Class", fontsize=16)

model.eval()
with torch.no_grad():
    for ax, class_name in zip(axes, class_names[:5]):
        # Sample one image from this class
        sample_row = val_full_df[val_full_df["class"] == class_name].sample(1).iloc[0]
        img = Image.open(sample_row["image"]).convert("RGB")
        img_tensor = transform(img).unsqueeze(0).to(device)
        
        # True vector from DataFrame, pred from model
        true_vec = torch.tensor(sample_row[[col for col in val_full_df.columns if isinstance(col, int)]].values.astype(np.float32))
        pred_vec = model(img_tensor).squeeze().cpu().numpy()
        
        xs = np.arange(100)
        for i in range(3):
            ax.plot(xs, true_vec[i * 100:(i + 1) * 100], color=colors[i], linestyle="--")
            ax.plot(xs, pred_vec[i * 100:(i + 1) * 100], color=colors[i])
        
        ax.set_title(class_name)
        ax.set_xticks([])
        ax.set_yticks([])
        ax.set_frame_on(False)

# Legend (only once)
from matplotlib.lines import Line2D
custom_lines = [
    Line2D([0], [0], color="black", linestyle="--", label="True"),
    Line2D([0], [0], color="black", linestyle="-", label="Predicted")
]
fig.legend(handles=custom_lines, loc="upper right", fontsize=10)
plt.tight_layout()
plt.subplots_adjust(top=0.85, right=0.95)
plt.show()

But let's go ahead and run both of these on the model with the best performance. That is, after this, the one with the best final validation accuracy we will show these:

'''
model = DenseNetToLandscape(out_dim=300)
optimizer = optim.Adam(model.parameters(), lr=1e-4)
loss_fn = nn.MSELoss()

model, train_losses, val_losses = train_landscape_model(
    model, train_loader, val_loader, optimizer, loss_fn, device, epochs=10
)

# Plotting loss
plt.figure(figsize=(8, 5))
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel("Epoch")
plt.ylabel("MSE Loss")
plt.title("Training vs Validation Loss")
plt.legend()
plt.grid(True)
plt.show()
'''

'''
LOVE THIS PLOT, DO THIS ON THE BEST PERFORMING MODEL!!!
'''


colors = ["red", "green", "blue"]
class_names = sorted(val_full_df["class"].unique())

fig, axes = plt.subplots(1, 5, figsize=(20, 4))
plt.suptitle("Predicted vs True Persistence Landscapes per Class", fontsize=16)

model.eval()
with torch.no_grad():
    for ax, class_name in zip(axes, class_names[:5]):
        # Sample one image from this class
        sample_row = val_full_df[val_full_df["class"] == class_name].sample(1).iloc[0]
        img = Image.open(sample_row["image"]).convert("RGB")
        img_tensor = transform(img).unsqueeze(0).to(device)
        
        # True vector from DataFrame, pred from model
        true_vec = torch.tensor(sample_row[[col for col in val_full_df.columns if isinstance(col, int)]].values.astype(np.float32))
        pred_vec = model(img_tensor).squeeze().cpu().numpy()
        
        xs = np.arange(100)
        for i in range(3):
            ax.plot(xs, true_vec[i * 100:(i + 1) * 100], color=colors[i], linestyle="--")
            ax.plot(xs, pred_vec[i * 100:(i + 1) * 100], color=colors[i])
        
        ax.set_title(class_name)
        ax.set_xticks([])
        ax.set_yticks([])
        ax.set_frame_on(False)

# Legend (only once)
from matplotlib.lines import Line2D
custom_lines = [
    Line2D([0], [0], color="black", linestyle="--", label="True"),
    Line2D([0], [0], color="black", linestyle="-", label="Predicted")
]
fig.legend(handles=custom_lines, loc="upper right", fontsize=10)
plt.tight_layout()
plt.subplots_adjust(top=0.85, right=0.95)
plt.show()

Oh and the code we're doing it after is this:

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

def evaluate_model(model, dataloader, device):
    model.eval()
    y_true, y_pred = [], []

    with torch.no_grad():
        for images, vectors in dataloader:
            images, vectors = images.to(device), vectors.to(device)
            preds = model(images)
            y_true.append(vectors.cpu().numpy())
            y_pred.append(preds.cpu().numpy())

    y_true = np.vstack(y_true)
    y_pred = np.vstack(y_pred)

    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)

    print(f"RMSE: {rmse:.4f}")
    print(f"MAE : {mae:.4f}")
    print(f"R²  : {r2:.4f}")

    return {"RMSE": rmse, "MAE": mae, "R2": r2}

from itertools import product

lrs = [5e-4, 1e-3]
decays = [0, 1e-5]
unfreeze_k = [0, 2]  # 0 = frozen, 2 = partial, 4 = more tuning


search_space = list(product(lrs, decays, unfreeze_k))

results = []

for lr, wd, k in search_space:
    print(f"\n--- Training: LR={lr}, WD={wd}, UnfreezeTopK={k} ---")

    model = DenseNetToLandscape(out_dim=300, unfreeze_top_k=k)
    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),
                           lr=lr, weight_decay=wd)

    model, train_losses, val_losses = train_landscape_model(
        model, train_loader, val_loader, optimizer, loss_fn, device, epochs = 5
    )

    metrics = evaluate_model(model, val_loader, device)
    metrics.update({
        "LR": lr,
        "WeightDecay": wd,
        "UnfreezeTopK": k,
        "TrainLossFinal": train_losses[-1],
        "ValLossFinal": val_losses[-1],
        "TrainLosses": train_losses,
        "ValLosses": val_losses
    })

    results.append(metrics)

And we also display a table, so right after this:

import pandas as pd
metrics_df = pd.DataFrame(results)
summary_df = metrics_df[["LR", "WeightDecay", "UnfreezeTopK", "RMSE", "MAE", "R2", "ValLossFinal"]]
summary_df.sort_values("ValLossFinal")

OUTPUT_20:

import torch.optim as optim
from itertools import product
import pandas as pd

# Initialize storage for grid search results and best model tracking
results = []
best_val_loss = float('inf')
best_model = None
best_train_losses = None
best_val_losses = None
best_hyperparams = {}

# Define hyperparameter search space
lrs = [5e-4, 1e-3]
decays = [0, 1e-5]
unfreeze_k = [0, 2]  # 0 = frozen, 2 = partial

search_space = list(product(lrs, decays, unfreeze_k))

for lr, wd, k in search_space:
    print(f"\n--- Training: LR={lr}, WD={wd}, UnfreezeTopK={k} ---")
    model = DenseNetToLandscape(out_dim=300, unfreeze_top_k=k)
    # Only optimize parameters that require grad
    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=wd)
    
    # Train for a few epochs (here 5 for speed)
    model, train_losses, val_losses = train_landscape_model(
        model, train_loader, val_loader, optimizer, loss_fn, device, epochs=5
    )
    
    # Evaluate model on validation set
    metrics = evaluate_model(model, val_loader, device)
    metrics.update({
        "LR": lr,
        "WeightDecay": wd,
        "UnfreezeTopK": k,
        "TrainLossFinal": train_losses[-1],
        "ValLossFinal": val_losses[-1],
        "TrainLosses": train_losses,
        "ValLosses": val_losses
    })
    
    results.append(metrics)
    
    # Check for best validation loss
    if val_losses[-1] < best_val_loss:
        best_val_loss = val_losses[-1]
        best_model = model  # best_model remains on GPU
        best_train_losses = train_losses
        best_val_losses = val_losses
        best_hyperparams = {"LR": lr, "WeightDecay": wd, "UnfreezeTopK": k}

# Create a summary table of grid search results
metrics_df = pd.DataFrame(results)
summary_df = metrics_df[["LR", "WeightDecay", "UnfreezeTopK", "RMSE", "MAE", "R2", "ValLossFinal"]]
summary_df = summary_df.sort_values("ValLossFinal")
print(summary_df)
print("Best Hyperparameters:", best_hyperparams)


import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
plt.plot(best_train_losses, label='Train Loss')
plt.plot(best_val_losses, label='Validation Loss')
plt.xlabel("Epoch", fontsize=14)
plt.ylabel("MSE Loss", fontsize=14)
plt.title("Training vs Validation Loss", fontsize=18, fontweight='bold', fontname='Times New Roman', color='black')
plt.legend(fontsize=12)
plt.grid(True)
plt.show()


from matplotlib.lines import Line2D
import numpy as np
from PIL import Image

colors = ["red", "green", "blue"]
class_names = sorted(val_full_df["class"].unique())

fig, axes = plt.subplots(1, 5, figsize=(20, 4))
plt.suptitle("Predicted vs True Persistence Landscapes per Class", fontsize=24, fontweight='bold', fontname='Times New Roman', color='black')

best_model.eval()
with torch.no_grad():
    for ax, class_name in zip(axes, class_names[:5]):
        # Sample one image from this class in validation set
        sample_row = val_full_df[val_full_df["class"] == class_name].sample(1).iloc[0]
        img = Image.open(sample_row["image"]).convert("RGB")
        img_tensor = transform(img).unsqueeze(0).to(device)
        
        # True vector (assumed to be stored in columns with integer names)
        true_vec = torch.tensor(sample_row[[col for col in val_full_df.columns if isinstance(col, int)]].values.astype(np.float32))
        pred_vec = best_model(img_tensor).squeeze().cpu().numpy()
        
        xs = np.arange(100)
        for i in range(3):
            ax.plot(xs, true_vec[i * 100:(i + 1) * 100], color=colors[i], linestyle="--")
            ax.plot(xs, pred_vec[i * 100:(i + 1) * 100], color=colors[i])
        
        ax.set_title(class_name.capitalize(), fontsize=14, fontweight='bold', fontname='Times New Roman', color='black')
        ax.set_xticks([])
        ax.set_yticks([])
        ax.set_frame_on(False)

# Custom legend: dashed for true, solid for predicted
custom_lines = [
    Line2D([0], [0], color="black", linestyle="--", label="True"),
    Line2D([0], [0], color="black", linestyle="-", label="Predicted")
]
fig.legend(handles=custom_lines, loc="upper right", fontsize=12)
plt.tight_layout()
plt.subplots_adjust(top=0.85, right=0.95)
plt.show()


QUERY_20:

# Create a summary table of grid search results
metrics_df = pd.DataFrame(results)
summary_df = metrics_df[["LR", "WeightDecay", "UnfreezeTopK", "RMSE", "MAE", "R2", "ValLossFinal"]]
summary_df = summary_df.sort_values("ValLossFinal")
print(summary_df)
print("Best Hyperparameters:", best_hyperparams)

This is not a nice table. A nicely formatted table (IMAGE??) I would rather the table be an image so I can potentially export it!

OUTPUT_20:

import matplotlib.pyplot as plt

# Create the summary table (using your already-created summary_df)
# Ensure summary_df is sorted as desired
summary_df = summary_df.sort_values("ValLossFinal")

# Create a figure and axis for the table
fig, ax = plt.subplots(figsize=(10, 4))
ax.axis('off')  # Hide the axis

# Create the table from the DataFrame
table = ax.table(
    cellText=summary_df.values,
    colLabels=summary_df.columns,
    cellLoc='center',
    loc='center'
)

# Set font sizes and scale the table
table.auto_set_font_size(False)
table.set_fontsize(12)
table.scale(1.2, 1.2)

# Apply styling: Bold header in Times New Roman
for (row, col), cell in table.get_celld().items():
    cell.set_edgecolor('black')
    if row == 0:
        cell.set_text_props(fontproperties={'weight': 'bold', 'family': 'Times New Roman'})
        cell.set_facecolor('#d3d3d3')  # light gray header background
    else:
        cell.set_text_props(fontproperties={'family': 'Times New Roman'})

# Set a title for the table (also styled)
plt.title("Grid Search Summary", fontsize=16, fontweight='bold', fontname='Times New Roman', color='black')

# Save the figure as a high-resolution image with a transparent background
plt.savefig("grid_search_summary.png", dpi=300, transparent=True, bbox_inches='tight')
plt.show()


QUERY_21:

import shap
import torch
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from torchvision import transforms
from torch.utils.data import DataLoader
from tqdm import tqdm

# ---------- SHAP Setup ----------

def setup_shap_explainer(model, background_loader, device):
    model.eval()
    model.to(device)
    background_batch = next(iter(background_loader))[0][:3].to(device)  # Use 3 background images for speed
    explainer = shap.GradientExplainer(model, background_batch)
    return explainer

# ---------- SHAP Value Computation ----------

def compute_shap_values(explainer, image_batch, top_k=1):
    """
    image_batch: Tensor of shape [B, 3, H, W], already on device.
    Returns SHAP values for top-k output dimensions.
    """
    shap_values = explainer.shap_values(image_batch, ranked_outputs=top_k)
    return shap_values  # List of length = top_k, each element is [B, 3, H, W]

# ---------- SHAP Visualization ----------

def visualize_shap_images(sample_paths, shap_values, transform, model_input_size=(3, 256, 256)):
    """
    Visualizes SHAP overlays.
      - sample_paths: List of image paths (length B)
      - shap_values: List with shape [top_k][B, 3, H, W]. Each element may be a torch.Tensor or a NumPy array.
    """
    fig, axes = plt.subplots(1, len(sample_paths), figsize=(4 * len(sample_paths), 4))
    if len(sample_paths) == 1:
        axes = [axes]  # Ensure iterable

    for i, (img_path, ax) in enumerate(zip(sample_paths, axes)):
        # Load image for display
        img = Image.open(img_path).convert("RGB")
        img_resized = img.resize((model_input_size[2], model_input_size[1]))
        img_np = np.array(img_resized) / 255.0  # Normalize for display

        # Retrieve SHAP values for this image from each output dimension
        per_output_vals = []
        for sv in shap_values:
            val = sv[i]
            if isinstance(val, torch.Tensor):
                val = val.detach().cpu().numpy()
            per_output_vals.append(val)
        
        # Check if all SHAP outputs have the same shape before stacking
        shapes = [np.array(val).shape for val in per_output_vals]
        if len(set(shapes)) == 1:
            aggregated = np.mean(np.abs(np.stack(per_output_vals)), axis=0)
        else:
            # Fallback: use the first output if shapes differ
            aggregated = np.abs(per_output_vals[0])
            
        shap_heatmap = np.mean(aggregated, axis=0)  # Average over channels → [H, W]

        ax.imshow(img_np)
        ax.imshow(shap_heatmap, cmap='hot', alpha=0.6)
        ax.axis("off")

    plt.tight_layout()
    plt.show()

# ---------- Load Background + Set Up Explainer ----------

background_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)
explainer = setup_shap_explainer(model, background_loader, device)

# ---------- Prepare (class, path) Sample List ----------

all_samples = []
for sample_class in ["buildings", "forest", "glacier", "mountain", "sea", "street"]:
    paths = train_df[train_df["class"] == sample_class]["image"].sample(5).tolist()
    all_samples.extend([(sample_class, p) for p in paths])

# ---------- Explain and Visualize (TQDM + Safe) ----------

for class_label, img_path in tqdm(all_samples, desc="Explaining images", unit="image"):
    image_tensor = transform(Image.open(img_path).convert("RGB")).unsqueeze(0).to(device)
    
    # Compute SHAP for top-1 output (faster)
    shap_values = compute_shap_values(explainer, image_tensor, top_k=1)
    
    # Visualize SHAP overlay for the image
    visualize_shap_images([img_path], shap_values, transform)

No need for TQDM. We need this to display in rows like:

'''
for sample_class in ["buildings", "forest", "glacier", "mountain", "sea", "street"]:
    fig, axes = plt.subplots(1, 5, figsize=(15, 3))
    sample_paths = train_df[train_df["class"] == sample_class]["image"].sample(5)

    for ax, img_path in zip(axes, sample_paths):
        img = Image.open(img_path)
        ax.imshow(img)
        ax.axis("off")
    plt.suptitle(f"Samples from class: {sample_class}")
    plt.show()
'''

for sample_class in ["buildings", "forest", "glacier", "mountain", "sea", "street"]:
    fig, axes = plt.subplots(1, 5, figsize=(15, 3))
    fig.set_dpi(300)
    fig.patch.set_alpha(0)  # Make the figure background transparent
    
    sample_paths = train_df[train_df["class"] == sample_class]["image"].sample(5)
    for ax, img_path in zip(axes, sample_paths):
        img = Image.open(img_path)
        ax.imshow(img)
        ax.axis("off")
    
    class_title = sample_class.capitalize()
    plt.suptitle(class_title, fontsize=24, fontweight='bold', fontname='Times New Roman', color='black')
    plt.show()

OUTPUT_21:

import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import torch

# For each class, sample 5 images, compute SHAP overlays in a batch, and plot them in one row.
for sample_class in ["buildings", "forest", "glacier", "mountain", "sea", "street"]:
    # Sample 5 image paths from the current class
    sample_paths = list(train_df[train_df["class"] == sample_class]["image"].sample(5))
    
    # Load and transform images into a batch
    batch_imgs = [transform(Image.open(p).convert("RGB")) for p in sample_paths]
    batch = torch.stack(batch_imgs).to(device)
    
    # Compute SHAP values for the batch (top-1 output for speed)
    shap_values = compute_shap_values(explainer, batch, top_k=1)
    
    # Create a figure with 5 subplots in one row
    fig, axes = plt.subplots(1, 5, figsize=(15, 3))
    fig.set_dpi(300)
    fig.patch.set_alpha(0)  # Transparent background
    
    # For each image in the batch, plot the original image with its SHAP heatmap overlay
    for i, ax in enumerate(axes):
        # Load the image for display (resize to a common size, e.g., 256x256)
        img = Image.open(sample_paths[i]).convert("RGB").resize((256, 256))
        img_np = np.array(img) / 255.0  # Normalize for display
        
        # Get the SHAP value for this image; since top_k=1, take the first element
        val = shap_values[0][i]
        if isinstance(val, torch.Tensor):
            val = val.detach().cpu().numpy()
        # For top_k=1, we can directly use the absolute SHAP value
        shap_overlay = np.mean(np.abs(val), axis=0)  # Average over channels to form a heatmap
        
        # Plot the original image and overlay the heatmap
        ax.imshow(img_np)
        ax.imshow(shap_overlay, cmap='hot', alpha=0.6)
        ax.axis("off")
    
    # Set a bold title in Times New Roman for the row, with first letter capitalized
    plt.suptitle(sample_class.capitalize(), fontsize=24, fontweight='bold', 
                 fontname='Times New Roman', color='black')
    plt.tight_layout()
    plt.show()
