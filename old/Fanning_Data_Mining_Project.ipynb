{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25c59a0-8db3-4d02-b7dd-68424e9fe7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import cripser as cr\n",
    "from gudhi.representations import Landscape\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import UnidentifiedImageError\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dad10af-bb28-4f0d-ab66-41ebcbb1d9f2",
   "metadata": {},
   "source": [
    "https://github.com/luangtatipsy/intel-image-classification/blob/master/101_result_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eab9e83-20f3-4e28-a2d3-0b3bff8056f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(directory):\n",
    "    records = []\n",
    "    for class_path in sorted([d for d in directory.iterdir() if d.is_dir()]):\n",
    "        for image_path in class_path.glob(\"*.jpg\"):\n",
    "            records.append({\"image\": str(image_path), \"class\": class_path.name})\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39de533d-9294-40b7-a391-89d17ce70aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = Path(\"../data/seg_train/seg_train\")\n",
    "train_df = dataloader(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8dfbd1-ede6-4057-b071-9f422e36d6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize = (12, 8))\n",
    "fig.suptitle(\"Intel Scene Classification Original Images\", fontsize=16)\n",
    "\n",
    "for ax, class_name in zip(axes.flat, sorted(train_df[\"class\"].unique())[:6]):\n",
    "    img_path = train_df[train_df[\"class\"] == class_name].iloc[0][\"image\"]\n",
    "    img = Image.open(img_path)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(class_name)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943050d7-048e-40c5-a969-4a87a8d06ab0",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814b71bd-bad5-4192-b969-cbd44fa6ee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_corrupt_images(df):\n",
    "    corrupt_paths = []\n",
    "    for path in df[\"image\"]:\n",
    "        try:\n",
    "            with Image.open(path) as img:\n",
    "                img.verify()\n",
    "        except (UnidentifiedImageError, IOError, OSError):\n",
    "            corrupt_paths.append(path)\n",
    "    return corrupt_paths\n",
    "\n",
    "corrupt_imgs = check_corrupt_images(train_df)\n",
    "print(\"Corrupt images:\", corrupt_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d45bcb0-6308-49a4-99d6-e96eef740867",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=train_df, x=\"class\")\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3d7acb-22ef-4bec-b950-e98e915f2f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_stds = []\n",
    "\n",
    "for idx, row in train_df.iterrows():\n",
    "    img = Image.open(row[\"image\"]).convert(\"L\")  # Convert to grayscale for std\n",
    "    std_val = np.std(np.array(img))\n",
    "    image_stds.append(std_val)\n",
    "\n",
    "train_df[\"std\"] = image_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24ea7b6-7e92-40c0-8dd2-9de575909220",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(train_df[\"std\"], bins=50, kde=True)\n",
    "plt.title(\"Image Standard Deviation Distribution\")\n",
    "plt.xlabel(\"Pixel Intensity Standard Deviation\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5877a055-e7c8-4fe0-97c6-e4c69aa76098",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_std_df = train_df.sort_values(\"std\").head(10)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for ax, (_, row) in zip(axes.flat, lowest_std_df.iterrows()):\n",
    "    img = Image.open(row[\"image\"])\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"std: {row['std']:.1f}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"10 Most Uniform Images (Lowest Std)\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.88)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3703dc-5423-4d66-9d3f-db3237d98ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"dimensions\"] = train_df[\"image\"].apply(lambda x: f\"{Image.open(x).size[0]}x{Image.open(x).size[1]}\")\n",
    "\n",
    "dimension_counts = train_df[\"dimensions\"].value_counts().reset_index()\n",
    "dimension_counts.columns = [\"dimensions\", \"count\"]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=dimension_counts, x=\"dimensions\", y=\"count\")\n",
    "plt.title(\"Image Dimension Distribution (Exact WxH)\")\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b63591-a032-4595-a611-548568502681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, parse the image size directly\n",
    "train_df[\"size\"] = train_df[\"image\"].apply(lambda x: Image.open(x).size)\n",
    "\n",
    "# Filter only 150x150\n",
    "train_df = train_df[train_df[\"size\"] == (150, 150)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Filtered dataset size: {train_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f92032-3c1a-4f85-9a41-267179ffeadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample_class in [\"buildings\", \"forest\", \"glacier\", \"mountain\", \"sea\", \"street\"]:\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "    fig.set_dpi(300)\n",
    "    fig.patch.set_alpha(0)  # Make the figure background transparent\n",
    "    \n",
    "    sample_paths = train_df[train_df[\"class\"] == sample_class][\"image\"].sample(5)\n",
    "    for ax, img_path in zip(axes, sample_paths):\n",
    "        img = Image.open(img_path)\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    class_title = sample_class.capitalize()\n",
    "    plt.suptitle(class_title, fontsize=24, fontweight='bold', fontname='Times New Roman', color='black')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855952dd-9911-44f3-a452-ef0b613bc7dc",
   "metadata": {},
   "source": [
    "# Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e3cfdb-3053-42bb-ac0c-a6b0f96e6a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_list, val_list = [], []\n",
    "\n",
    "for class_name, group in train_df.groupby(\"class\"):\n",
    "    train_split, val_split = train_test_split(\n",
    "        group, test_size=0.2, random_state=42, shuffle=True, stratify=None\n",
    "    )\n",
    "    train_list.append(train_split)\n",
    "    val_list.append(val_split)\n",
    "\n",
    "train_split_df = pd.concat(train_list).reset_index(drop=True)\n",
    "val_split_df = pd.concat(val_list).reset_index(drop=True)\n",
    "\n",
    "print(f\"Train split: {train_split_df.shape}, Val split: {val_split_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56325b34-1d42-4ac6-97b8-5be2c292283b",
   "metadata": {},
   "source": [
    "https://github.com/shizuo-kaji/CubicalRipser_3dim/blob/master/demo/cubicalripser.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eb2f00-8da1-4bd1-a2ab-4a30d93f5f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_persistence_diagrams(img2d):\n",
    "    pd_v = cr.computePH(img2d)\n",
    "    pd_v = pd_v[:, :3]\n",
    "    return [pd_v[pd_v[:, 0] == i][:, 1:] for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ba0c9-6fba-43c2-81dc-7c54acfdc0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagram_to_landscape(diag, num_landscapes = 3, resolution = 100):\n",
    "    L = Landscape(num_landscapes = num_landscapes, resolution = resolution)\n",
    "    return L.fit_transform([diag])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb53dd27-6be6-474a-b2a7-db886c9e334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_image_topology_vector(img_path, dims = [1], num_landscapes = 3, resolution = 100):\n",
    "    img2d = np.array(Image.open(img_path).convert(\"L\"))\n",
    "    pds_v = compute_persistence_diagrams(img2d)\n",
    "    vecs = [diagram_to_landscape(pds_v[dim], num_landscapes, resolution) for dim in dims]\n",
    "    return np.concatenate(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f8a5a6-5c93-4eec-8831-e33df93842ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "train_vectors = [compute_image_topology_vector(row[\"image\"], dims=[1], num_landscapes=3, resolution=100)\n",
    "                 for _, row in tqdm(train_split_df.iterrows(), total=len(train_split_df))]\n",
    "train_vector_df = pd.DataFrame(train_vectors)\n",
    "train_full_df = pd.concat([train_split_df, train_vector_df], axis=1)\n",
    "\n",
    "val_vectors = [compute_image_topology_vector(row[\"image\"], dims=[1], num_landscapes=3, resolution=100)\n",
    "               for _, row in tqdm(val_split_df.iterrows(), total=len(val_split_df))]\n",
    "val_vector_df = pd.DataFrame(val_vectors)\n",
    "val_full_df = pd.concat([val_split_df, val_vector_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896f167b-9890-492f-b3e3-bdef29b779af",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "fig.suptitle(\"Persistence Diagram of Intel Scene Classification Images\", \n",
    "             fontsize=24, fontweight='bold', fontname='Times New Roman', color='black')\n",
    "\n",
    "for ax, class_name in zip(axes.flat, sorted(train_df[\"class\"].unique())[:6]):\n",
    "    img_path = train_df[train_df[\"class\"] == class_name].iloc[0][\"image\"]\n",
    "    img = np.array(Image.open(img_path).convert(\"L\"))\n",
    "    pdgms = compute_persistence_diagrams(img)\n",
    "    diag = pdgms[1]\n",
    "    ax.scatter(diag[:, 0], diag[:, 1], s=10)\n",
    "    ax.plot([0, diag[:, 1].max()], [0, diag[:, 1].max()], 'r--', linewidth=1)\n",
    "    # Set subplot title in bold Times New Roman with first letter capitalized\n",
    "    ax.set_title(class_name.capitalize(), \n",
    "                 fontdict={'fontsize':16, 'fontweight':'bold', 'fontname':'Times New Roman', 'color':'black'})\n",
    "    ax.axis(\"equal\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca21edb-3105-4bf6-abff-1d76d45796b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "fig.suptitle(\"Persistence Landscapes of Intel Scene Classification Images\", \n",
    "             fontsize=24, fontweight='bold', fontname='Times New Roman', color='black')\n",
    "\n",
    "for ax, class_name in zip(axes.flat, sorted(train_df[\"class\"].unique())[:6]):\n",
    "    img_path = train_df[train_df[\"class\"] == class_name].iloc[0][\"image\"]\n",
    "    vec = compute_image_topology_vector(img_path, dims=[1], num_landscapes=3, resolution=100)\n",
    "    for i in range(3):\n",
    "        ax.plot(vec[i * 100:(i + 1) * 100])\n",
    "    ax.set_title(class_name.capitalize(), \n",
    "                 fontdict={'fontsize':16, 'fontweight':'bold', 'fontname':'Times New Roman', 'color':'black'})\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310896aa-07eb-497d-8864-784269661cf1",
   "metadata": {},
   "source": [
    "https://github.com/navpreetnp7/Image-Classification-using-Densenet/blob/main/SynergyLabs.Task.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06f2d78-749b-4cd4-b95e-4c798f6ded70",
   "metadata": {},
   "source": [
    "# DenseNet Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de047cf1-b68a-464b-b716-99930228b9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageToLandscapeDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform = None):\n",
    "        self.df = dataframe\n",
    "        self.transform = transform\n",
    "        self.vec_cols = [col for col in dataframe.columns if isinstance(col, int)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row[\"image\"]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        vec = torch.tensor(row[self.vec_cols].values.astype(np.float32))\n",
    "        return img, vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266a1d3b-59bf-42fc-ba07-93cc7ffd70a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNetToLandscape(nn.Module):\n",
    "    def __init__(self, out_dim, unfreeze_top_k=0):\n",
    "        super().__init__()\n",
    "        self.base = models.densenet121(pretrained=True)\n",
    "        \n",
    "        # Freeze all layers initially\n",
    "        for param in self.base.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Optionally unfreeze top-k layers of features\n",
    "        if unfreeze_top_k > 0:\n",
    "            layers = list(self.base.features.children())[::-1]\n",
    "            count = 0\n",
    "            for layer in layers:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "                count += 1\n",
    "                if count >= unfreeze_top_k:\n",
    "                    break\n",
    "\n",
    "        in_features = self.base.classifier.in_features\n",
    "        self.base.classifier = nn.Linear(in_features, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46aebbc-2e53-4aeb-841c-d82f58a76a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_landscape_model(model, train_loader, val_loader, optimizer, loss_fn, device, epochs=10):\n",
    "    model.to(device)\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        # Wrap the train_loader in tqdm\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\", leave=False)\n",
    "        for images, vectors in pbar:\n",
    "            images, vectors = images.to(device), vectors.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(images)\n",
    "            loss = loss_fn(preds, vectors)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\", leave=False)\n",
    "            for images, vectors in pbar:\n",
    "                images, vectors = images.to(device), vectors.to(device)\n",
    "                preds = model(images)\n",
    "                loss = loss_fn(preds, vectors)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
    "\n",
    "    return model, train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772c740c-8b21-4cb4-bead-7819ec6652bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = ImageToLandscapeDataset(train_full_df, transform=transform)\n",
    "val_dataset = ImageToLandscapeDataset(val_full_df, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54691701-cf14-47fd-a2c9-bae45a7a73ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fix SHAP incompatibility by disabling in-place ReLUs\n",
    "def set_relu_to_non_inplace(model):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, torch.nn.ReLU):\n",
    "            module.inplace = False\n",
    "\n",
    "\n",
    "model = DenseNetToLandscape(out_dim=300)  # 3 landscapes x 100 resolution\n",
    "set_relu_to_non_inplace(model)            \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49846203-3c51-4488-b4b9-3cc502af04b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, vectors in dataloader:\n",
    "            images, vectors = images.to(device), vectors.to(device)\n",
    "            preds = model(images)\n",
    "            y_true.append(vectors.cpu().numpy())\n",
    "            y_pred.append(preds.cpu().numpy())\n",
    "\n",
    "    y_true = np.vstack(y_true)\n",
    "    y_pred = np.vstack(y_pred)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE : {mae:.4f}\")\n",
    "    print(f\"R²  : {r2:.4f}\")\n",
    "\n",
    "    return {\"RMSE\": rmse, \"MAE\": mae, \"R2\": r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ca33c3-da0d-4dd0-802f-415780a92f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize storage for grid search results and best model tracking\n",
    "results = []\n",
    "best_val_loss = float('inf')\n",
    "best_model = None\n",
    "best_train_losses = None\n",
    "best_val_losses = None\n",
    "best_hyperparams = {}\n",
    "\n",
    "# Define hyperparameter search space\n",
    "lrs = [5e-4, 1e-3]\n",
    "decays = [0, 1e-5]\n",
    "unfreeze_k = [0, 2]  # 0 = frozen, 2 = partial\n",
    "\n",
    "search_space = list(product(lrs, decays, unfreeze_k))\n",
    "\n",
    "for lr, wd, k in search_space:\n",
    "    print(f\"\\n--- Training: LR={lr}, WD={wd}, UnfreezeTopK={k} ---\")\n",
    "    model = DenseNetToLandscape(out_dim=300, unfreeze_top_k=k)\n",
    "    # Only optimize parameters that require grad\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=wd)\n",
    "    \n",
    "    # Train for a few epochs (here 5 for speed)\n",
    "    model, train_losses, val_losses = train_landscape_model(\n",
    "        model, train_loader, val_loader, optimizer, loss_fn, device, epochs=10\n",
    "    )\n",
    "    \n",
    "    # Evaluate model on validation set\n",
    "    metrics = evaluate_model(model, val_loader, device)\n",
    "    metrics.update({\n",
    "        \"LR\": lr,\n",
    "        \"WeightDecay\": wd,\n",
    "        \"UnfreezeTopK\": k,\n",
    "        \"TrainLossFinal\": train_losses[-1],\n",
    "        \"ValLossFinal\": val_losses[-1],\n",
    "        \"TrainLosses\": train_losses,\n",
    "        \"ValLosses\": val_losses\n",
    "    })\n",
    "    \n",
    "    results.append(metrics)\n",
    "    \n",
    "    # Check for best validation loss\n",
    "    if val_losses[-1] < best_val_loss:\n",
    "        best_val_loss = val_losses[-1]\n",
    "        best_model = model  # best_model remains on GPU\n",
    "        best_train_losses = train_losses\n",
    "        best_val_losses = val_losses\n",
    "        best_hyperparams = {\"LR\": lr, \"WeightDecay\": wd, \"UnfreezeTopK\": k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c01d41-f97e-4434-a4ae-9b6657102f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "metrics_df = pd.DataFrame(results)\n",
    "summary_df = metrics_df[[\"LR\", \"WeightDecay\", \"UnfreezeTopK\", \"RMSE\", \"MAE\", \"R2\", \"ValLossFinal\"]]\n",
    "summary_df = summary_df.sort_values(\"ValLossFinal\")\n",
    "\n",
    "# Helper function to format numeric values: 5 significant figures and no scientific notation.\n",
    "def format_val(val):\n",
    "    try:\n",
    "        val = float(val)\n",
    "        s = f\"{val:.5g}\"  # 5 significant digits\n",
    "        if \"e\" in s or \"E\" in s:  # Force plain decimal if scientific notation is used\n",
    "            s = f\"{val:.5f}\"\n",
    "        return s\n",
    "    except:\n",
    "        return str(val)\n",
    "\n",
    "# Format each cell in summary_df (assumed to be already defined and sorted)\n",
    "formatted_values = summary_df.applymap(lambda x: format_val(x)).values.tolist()\n",
    "\n",
    "# Create a figure and axis for the table\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.axis('off')  # Hide the axis\n",
    "\n",
    "# Create the table using the formatted values and the DataFrame's columns as headers\n",
    "table = ax.table(\n",
    "    cellText=formatted_values,\n",
    "    colLabels=summary_df.columns,\n",
    "    cellLoc='center',\n",
    "    loc='center'\n",
    ")\n",
    "\n",
    "# Set font sizes and scale the table\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12)\n",
    "table.scale(1.2, 1.2)\n",
    "\n",
    "# Apply styling: Bold header in Times New Roman with a light gray background\n",
    "for (row, col), cell in table.get_celld().items():\n",
    "    cell.set_edgecolor('black')\n",
    "    if row == 0:\n",
    "        cell.set_text_props(fontproperties={'weight': 'bold', 'family': 'Times New Roman'})\n",
    "        cell.set_facecolor('#d3d3d3')  # Light gray for header\n",
    "    else:\n",
    "        cell.set_text_props(fontproperties={'family': 'Times New Roman'})\n",
    "\n",
    "# Set a title for the table\n",
    "plt.title(\"Grid Search Summary\", fontsize=16, fontweight='bold', fontname='Times New Roman', color='black')\n",
    "\n",
    "# Save the figure as a high-resolution image with a transparent background\n",
    "plt.savefig(\"grid_search_summary.png\", dpi=300, transparent=True, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3a4a24-1401-4e9f-9519-fe48366464b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(best_train_losses, label='Train Loss')\n",
    "plt.plot(best_val_losses, label='Validation Loss')\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"MSE Loss\", fontsize=14)\n",
    "plt.title(\"Training vs Validation Loss\", fontsize=18, fontweight='bold', fontname='Times New Roman', color='black')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe40f3ae-eff0-4c15-b599-96c06e1d64a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "colors = [\"red\", \"green\", \"blue\"]\n",
    "class_names = sorted(val_full_df[\"class\"].unique())\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "plt.suptitle(\"Predicted vs True Persistence Landscapes\", fontsize=24, fontweight='bold', fontname='Times New Roman', color='black')\n",
    "\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    for ax, class_name in zip(axes, class_names[:5]):\n",
    "        # Sample one image from this class in validation set\n",
    "        sample_row = val_full_df[val_full_df[\"class\"] == class_name].sample(1).iloc[0]\n",
    "        img = Image.open(sample_row[\"image\"]).convert(\"RGB\")\n",
    "        img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "        \n",
    "        # True vector (assumed to be stored in columns with integer names)\n",
    "        true_vec = torch.tensor(sample_row[[col for col in val_full_df.columns if isinstance(col, int)]].values.astype(np.float32))\n",
    "        pred_vec = best_model(img_tensor).squeeze().cpu().numpy()\n",
    "        \n",
    "        xs = np.arange(100)\n",
    "        for i in range(3):\n",
    "            ax.plot(xs, true_vec[i * 100:(i + 1) * 100], color=colors[i], linestyle=\"--\")\n",
    "            ax.plot(xs, pred_vec[i * 100:(i + 1) * 100], color=colors[i])\n",
    "        \n",
    "        ax.set_title(class_name.capitalize(), fontsize=14, fontweight='bold', fontname='Times New Roman', color='black')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_frame_on(False)\n",
    "\n",
    "# Custom legend: dashed for true, solid for predicted\n",
    "custom_lines = [\n",
    "    Line2D([0], [0], color=\"black\", linestyle=\"--\", label=\"True\"),\n",
    "    Line2D([0], [0], color=\"black\", linestyle=\"-\", label=\"Predicted\")\n",
    "]\n",
    "fig.legend(handles=custom_lines, loc=\"upper right\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85, right=0.95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d4fdae-3e08-492f-932c-a9078037c95b",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2817859f-a78f-427c-90cd-7b9343218a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torchvision.models import densenet121\n",
    "\n",
    "class SHAPSafeDenseNet121(torch.nn.Module):\n",
    "    def __init__(self, out_dim):\n",
    "        super().__init__()\n",
    "        base = densenet121(pretrained=True)\n",
    "\n",
    "        # Freeze all layers\n",
    "        for param in base.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.features = base.features\n",
    "        self.pool = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = torch.nn.Linear(base.classifier.in_features, out_dim)\n",
    "\n",
    "        # Unfreeze final classifier layer\n",
    "        for param in self.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        features = F.relu(features.clone(), inplace=False)  # ✅ clone to avoid inplace error on views\n",
    "        out = self.pool(features)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71164393-8259-458c-91b8-9aa93ecb7069",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SHAPSafeDenseNet121(out_dim=300)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1302fd-cf85-455e-b0bb-d91c578c71bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------- SHAP Setup ----------\n",
    "\n",
    "def setup_shap_explainer(model, background_loader, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    background_batch = next(iter(background_loader))[0][:3].to(device)  # Use 3 background images for speed\n",
    "    explainer = shap.GradientExplainer(model, background_batch)\n",
    "    return explainer\n",
    "\n",
    "# ---------- SHAP Value Computation ----------\n",
    "\n",
    "def compute_shap_values(explainer, image_batch, top_k=1):\n",
    "    \"\"\"\n",
    "    image_batch: Tensor of shape [B, 3, H, W], already on device.\n",
    "    Returns SHAP values for top-k output dimensions.\n",
    "    \"\"\"\n",
    "    shap_values = explainer.shap_values(image_batch, ranked_outputs=top_k)\n",
    "    return shap_values  # List of length = top_k, each element is [B, 3, H, W]\n",
    "\n",
    "# ---------- SHAP Visualization ----------\n",
    "\n",
    "def visualize_shap_images(sample_paths, shap_values, transform, model_input_size=(3, 256, 256)):\n",
    "    \"\"\"\n",
    "    Visualizes SHAP overlays.\n",
    "      - sample_paths: List of image paths (length B)\n",
    "      - shap_values: List with shape [top_k][B, 3, H, W]. Each element may be a torch.Tensor or a NumPy array.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, len(sample_paths), figsize=(4 * len(sample_paths), 4))\n",
    "    if len(sample_paths) == 1:\n",
    "        axes = [axes]  # Ensure iterable\n",
    "\n",
    "    for i, (img_path, ax) in enumerate(zip(sample_paths, axes)):\n",
    "        # Load image for display\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_resized = img.resize((model_input_size[2], model_input_size[1]))\n",
    "        img_np = np.array(img_resized) / 255.0  # Normalize for display\n",
    "\n",
    "        # Retrieve SHAP values for this image from each output dimension\n",
    "        per_output_vals = []\n",
    "        for sv in shap_values:\n",
    "            val = sv[i]\n",
    "            if isinstance(val, torch.Tensor):\n",
    "                val = val.detach().cpu().numpy()\n",
    "            per_output_vals.append(val)\n",
    "        \n",
    "        # Check if all SHAP outputs have the same shape before stacking\n",
    "        shapes = [np.array(val).shape for val in per_output_vals]\n",
    "        if len(set(shapes)) == 1:\n",
    "            aggregated = np.mean(np.abs(np.stack(per_output_vals)), axis=0)\n",
    "        else:\n",
    "            # Fallback: use the first output if shapes differ\n",
    "            aggregated = np.abs(per_output_vals[0])\n",
    "            \n",
    "        shap_heatmap = np.mean(aggregated, axis=0)  # Average over channels → [H, W]\n",
    "\n",
    "        ax.imshow(img_np)\n",
    "        ax.imshow(shap_heatmap, cmap='hot', alpha=0.6)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show\n",
    "\n",
    "\n",
    "# ---------- Load Background + Set Up Explainer ----------\n",
    "\n",
    "background_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "explainer = setup_shap_explainer(model, background_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6977005-550f-4816-ab30-6f3215628b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# For each class, sample 5 images, compute SHAP overlays in a batch, and plot them in one row.\n",
    "for sample_class in [\"buildings\", \"forest\", \"glacier\", \"mountain\", \"sea\", \"street\"]:\n",
    "    # Sample 5 image paths from the current class\n",
    "    sample_paths = list(train_df[train_df[\"class\"] == sample_class][\"image\"].sample(5))\n",
    "    \n",
    "    # Load and transform images into a batch\n",
    "    batch_imgs = [transform(Image.open(p).convert(\"RGB\")) for p in sample_paths]\n",
    "    batch = torch.stack(batch_imgs).to(device)\n",
    "    \n",
    "    # Compute SHAP values for the batch (top-1 output for speed)\n",
    "    shap_values = compute_shap_values(explainer, batch, top_k=1)\n",
    "    \n",
    "    # Create a figure with 5 subplots in one row\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "    fig.set_dpi(300)\n",
    "    fig.patch.set_alpha(0)  # Transparent background\n",
    "    \n",
    "    # For each image in the batch, plot the original image with its SHAP heatmap overlay\n",
    "    for i, ax in enumerate(axes):\n",
    "        # Load the image for display (resize to a common size, e.g., 256x256)\n",
    "        img = Image.open(sample_paths[i]).convert(\"RGB\").resize((256, 256))\n",
    "        img_np = np.array(img) / 255.0  # Normalize for display\n",
    "        \n",
    "        # Get the SHAP value for this image; since top_k=1, take the first element\n",
    "        val = shap_values[0][i]\n",
    "        if isinstance(val, torch.Tensor):\n",
    "            val = val.detach().cpu().numpy()\n",
    "        # For top_k=1, we can directly use the absolute SHAP value\n",
    "        shap_overlay = np.mean(np.abs(val), axis=0)  # Average over channels to form a heatmap\n",
    "        \n",
    "        # Plot the original image and overlay the heatmap\n",
    "        ax.imshow(img_np)\n",
    "        ax.imshow(shap_overlay, cmap='hot', alpha=0.6)\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    # Set a bold title in Times New Roman for the row, with first letter capitalized\n",
    "    plt.suptitle(sample_class.capitalize(), fontsize=24, fontweight='bold', \n",
    "                 fontname='Times New Roman', color='black')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e430119c-a32b-4517-b81e-17ee1a1852f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
